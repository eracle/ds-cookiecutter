{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nesta Data Science Cookiecutter \u00b6 A standard project structure for reproducible and collaborative data science projects @ Nesta. High-level aims \u00b6 Enable data scientists @ Nesta to work with each other Increase reliability of data science @ Nesta Make our projects more reproducible Allow data scientists to hand off to data engineers Give data scientists easy access to cloud computing (Light) Data version control Increase the value of codebases, and accompanying documentation/reports to stakeholders Keep reporting of results in sync with codebase Facilitate code-reuse and refactoring into *_DAPS and ds-utils by reducing coupling and increasing code quality Make code easier to understand Whilst retaining as much agility and freedom to explore as possible.","title":"Home"},{"location":"#nesta-data-science-cookiecutter","text":"A standard project structure for reproducible and collaborative data science projects @ Nesta.","title":"Nesta Data Science Cookiecutter"},{"location":"#high-level-aims","text":"Enable data scientists @ Nesta to work with each other Increase reliability of data science @ Nesta Make our projects more reproducible Allow data scientists to hand off to data engineers Give data scientists easy access to cloud computing (Light) Data version control Increase the value of codebases, and accompanying documentation/reports to stakeholders Keep reporting of results in sync with codebase Facilitate code-reuse and refactoring into *_DAPS and ds-utils by reducing coupling and increasing code quality Make code easier to understand Whilst retaining as much agility and freedom to explore as possible.","title":"High-level aims"},{"location":"examples/","text":"Example project structures \u00b6 Coming soon.","title":"Example project structures"},{"location":"examples/#example-project-structures","text":"Coming soon.","title":"Example project structures"},{"location":"faq/","text":"FAQ \u00b6 Please submit questions as a Github issue with the label \"question\". What customisations can I make when setting up the cookiecutter? \u00b6 You should be careful about modifying the cookiecutter - big changes defeat the point of having a standard project template. The other side of the equation is having a hackable standard starting point - the cookiecutter aims to be small and simple enough to understand which allows customisation based on needs that can't be centrally anticipated (but we encourage those customisations to be fed back to us as an issue). Some things that might be reasonable: Remove initial dependencies you know you won't need Add dependencies you know you'll need Extra documentation Add subfolder structure right away to help co-ordinate work Replace conda with a different virtual environment / dependency management tool Where should I save models? \u00b6 If it's a pre-trained model someone has sent you: inputs/models/ If it's a model you have trained: outputs/models/","title":"FAQ"},{"location":"faq/#faq","text":"Please submit questions as a Github issue with the label \"question\".","title":"FAQ"},{"location":"faq/#what-customisations-can-i-make-when-setting-up-the-cookiecutter","text":"You should be careful about modifying the cookiecutter - big changes defeat the point of having a standard project template. The other side of the equation is having a hackable standard starting point - the cookiecutter aims to be small and simple enough to understand which allows customisation based on needs that can't be centrally anticipated (but we encourage those customisations to be fed back to us as an issue). Some things that might be reasonable: Remove initial dependencies you know you won't need Add dependencies you know you'll need Extra documentation Add subfolder structure right away to help co-ordinate work Replace conda with a different virtual environment / dependency management tool","title":"What customisations can I make when setting up the cookiecutter?"},{"location":"faq/#where-should-i-save-models","text":"If it's a pre-trained model someone has sent you: inputs/models/ If it's a model you have trained: outputs/models/","title":"Where should I save models?"},{"location":"guidelines/","text":"Technical and working style guidelines \u00b6 Challenging this documentation is encouraged. Please set up an issue for challenges, additions or corrections. For explanations, please consult the dev Slack channel In this document we set out some basic \"tips\" (either mandatory or encouraged) which should guide your way of working, regardless of the task. Foreword python is not pandas Design patterns Programming Critical thinking Naming conventions Spaces and spacing Comments and docs Foreword \u00b6 In advance, we recommend installing the autoformatter black in your IDE. In future we will use flake8 (or similar) for automatically checking our python codebases. Hopefully the conventions laid out here are the easy and intuitive set of pep8 . Code reviewers: it is on you to ensure that this style guide has been followed: there are no points for being lenient, but there [non-redeemable] points for being opinionated! We should all feel pressured into making sure that our code meets an acceptable standard. python is not pandas \u00b6 tldr; Using pandas as a means to perform transformations or calculations on your data should be avoided, unless it clearly simplifies the logic and readability of your code. That is not so say that you should not use pandas , but rather that you justify to yourself that pandas isn't making your life harder in lieu of using standard python tools. We appreciate that pandas is a gateway into python programming for many people, and for that reason it becomes habitual way of coding. However... code containing lots of pandas operations are almost impossible to review, and therefore have the capacity to accumulate vast numbers of bugs. In general, pandas makes column-wise operations and IO (reading/writing files) dead easy. That said, pandas column-wise operations are inherited from numpy , and numpy is generally accepted in the place of dataframes. pandas is enormous, in many ways. If it can be omitted from your code then you can make big savings in terms of memory usage and requirements clashes, and even CPU time. Instead of Googling how to achieve something in pandas with an obscure chaining of functions, break the problem down and solve it yourself. It is highly unlikely that the pandas approach to reshaping your data will beat using tools from numpy , itertools , functools and toolz , even if you switch to representing data in numpy arrays or even as list of dict ( [{'value': 21}, {'value': 45}] ). If you would like guidance, tips or ideas on how to un panda your code then ask on the dev Slack Channel - we're all here to help! Design patterns \u00b6 Favour the following design patterns, in this order: Functional programming: using functions to do one thing well, not altering the original data. Modular code: using functions to eliminate duplication, improve readability and reduce indentation. Object-oriented programming: Generally avoid, unless you are customising an API (for example DataFrame ) or defining your own API. If you are not, at least, adhering to a modular style then you have gone very wrong. You should implement unit tests for each of your functions, something which is generally more tricky for object-oriented programming. Programming \u00b6 Mandatory \u00b6 NB: eventually these checks will be automatic Don't compare boolean values to True or False . Favour is not condition over not condition is Don't compare a value to None ( value == None ), always favour value is None Encouraged \u00b6 Favour logging over print Favour using configuration files, or (faster/lazier/less good/ok) GLOBAL_VARIABLES near the top of your code, rather than repeated use of hard-coded variables in your code, particularly when with URL and file path variables like s3://path/to/some/fixed/place , but also for repeated numeric hyperparameters. Critical thinking \u00b6 The following questions should be going through your mind when developing your code. If you don't quite understand the wording or intention of the following questions then we encourage you to ask in the dev Slack channel! Surely this simple problem already has an elegant solution? How many copies of the data am I making in memory? Where do my variables go out of scope? How can I avoid creating a new variable at all costs? (think iterator, scope, lru_cache ) Have I made sure that I only run expensive or time consuming processes as few times as possible? Naming conventions \u00b6 Mandatory \u00b6 Functions / methods: function , my_function (snake case) Variables / attributes: variable , my_var (snake case) Class: Model , MyClass (camel case) Module / file names / directory names: module , file_name.py , dir_name (camel case) Global* / constants: A_GLOBAL_VARIABLE (screaming snake case) * here we use \"Global\" to mean constants in scope at the module level, not the global level. Don't use global , ever. Encouraged \u00b6 Keep all names as short and descriptive as possible. Variable names such as x or df are highly discouraged unless they are genuinely representing abstract concepts. Favour good naming conventions over helpful comments Spaces and spacing \u00b6 NB: that using the autoformatter black in your IDE will resolve almost all of the following Encouraged \u00b6 Use the absolute minimum number of indents of your code. You can achieve this by writing modular code, and inverting logic, for example: def something ( args ): for item_collection in args : # 1 tab if item_collection . exists (): # 2 tabs the_sum = 0 # 3 tabs for item in item_collection : the_sum += item . value () # 4 tabs print ( the_sum ) can become: def sum_values ( item_collection ): the_sum = sum ( item . value () for item in item_collection ) print ( the_sum ) def something ( args ): for item_collection in args : # 1 tab if not item_collection . exists (): # 2 tabs continue # 3 tabs, buts unindents following lines sum_values ( item_collection ) # 2 tabs Put a space before starting block comments # like this , #not this Inline comments need two spaces before them a = 2 # like this Keep lines to 88 (officially 79) characters or less. You can achieve this by utilising other parts of this guideline, particularly with regards to creating modular code. Splitting over multiple lines is, of course, permissible so long as it doesn't conflict with legibility. When declaring default values, never put spaces around operators like = , i.e def this_is_ok(param=1) , def this_is_NOT_ok(param = 1) . Otherwise, all operators must always have a single space on either side. Separate function and class arguments with a comma and a space, i.e. do_thing(1, b=2) and not do_thing(1,b=2) . Comments and docs \u00b6 Mandatory \u00b6 At least a basic docstring is required for every function/method and class Full, explanatory docstrings are required for all function/methods and classes if it will be used in the main body of a code routine. Use Google-style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html If you are using type hints then then you can write e.g. my_arg (str): description as simply my_arg: description . Encouraged \u00b6 Don't state the obvious in comments Before writing a comment, consider whether that information would be better be encoded in a useful variable or function name. Workflow \u00b6 This builds on a much greater body of work, laid out in nestauk/github_support . For avoidance of doubt, branches must be linked to a GitHub issue and named accordingly: { GitHub issue number } _ { tinyLowerCamelDescription } For example 14_readme , which indicates that this branch refered to this issue . You should generally favour having a dev branch, in addition to your main ( master ) branch. Never commit to dev , master or main . Only pull requests from branches named {GitHub issue number}_{tinyLowerCamelDescription} should ever be accepted. Please make all PRs and issues reasonably small: they should be trying to achieve roughly one task. Inevitably some simple tasks spawn large numbers of utils, and sometimes these detract from the original PR. In this case, you should stack an new PR on top of your \"base\" PR, for example as {GitHub issue number}_{differentLowerCamelDescription} . In this case the PR / Git merging tree will look like: dev <-- 123_originalThing <-- 423_differentThing <-- 578_anotherDifferentThing We can then merge the PR 123_originalThing into dev , then 423_differentThing into dev (after calling git merge dev on 423_differentThing ), etc until the chain is merged entirely. The nominated reviewer should review the entire chain, before the merge can go ahead. PRs should only be merged if all tests and a review has been signed off.","title":"Python guidelines"},{"location":"guidelines/#technical-and-working-style-guidelines","text":"Challenging this documentation is encouraged. Please set up an issue for challenges, additions or corrections. For explanations, please consult the dev Slack channel In this document we set out some basic \"tips\" (either mandatory or encouraged) which should guide your way of working, regardless of the task. Foreword python is not pandas Design patterns Programming Critical thinking Naming conventions Spaces and spacing Comments and docs","title":"Technical and working style guidelines"},{"location":"guidelines/#foreword","text":"In advance, we recommend installing the autoformatter black in your IDE. In future we will use flake8 (or similar) for automatically checking our python codebases. Hopefully the conventions laid out here are the easy and intuitive set of pep8 . Code reviewers: it is on you to ensure that this style guide has been followed: there are no points for being lenient, but there [non-redeemable] points for being opinionated! We should all feel pressured into making sure that our code meets an acceptable standard.","title":"Foreword"},{"location":"guidelines/#python-is-not-pandas","text":"tldr; Using pandas as a means to perform transformations or calculations on your data should be avoided, unless it clearly simplifies the logic and readability of your code. That is not so say that you should not use pandas , but rather that you justify to yourself that pandas isn't making your life harder in lieu of using standard python tools. We appreciate that pandas is a gateway into python programming for many people, and for that reason it becomes habitual way of coding. However... code containing lots of pandas operations are almost impossible to review, and therefore have the capacity to accumulate vast numbers of bugs. In general, pandas makes column-wise operations and IO (reading/writing files) dead easy. That said, pandas column-wise operations are inherited from numpy , and numpy is generally accepted in the place of dataframes. pandas is enormous, in many ways. If it can be omitted from your code then you can make big savings in terms of memory usage and requirements clashes, and even CPU time. Instead of Googling how to achieve something in pandas with an obscure chaining of functions, break the problem down and solve it yourself. It is highly unlikely that the pandas approach to reshaping your data will beat using tools from numpy , itertools , functools and toolz , even if you switch to representing data in numpy arrays or even as list of dict ( [{'value': 21}, {'value': 45}] ). If you would like guidance, tips or ideas on how to un panda your code then ask on the dev Slack Channel - we're all here to help!","title":"python is not pandas"},{"location":"guidelines/#design-patterns","text":"Favour the following design patterns, in this order: Functional programming: using functions to do one thing well, not altering the original data. Modular code: using functions to eliminate duplication, improve readability and reduce indentation. Object-oriented programming: Generally avoid, unless you are customising an API (for example DataFrame ) or defining your own API. If you are not, at least, adhering to a modular style then you have gone very wrong. You should implement unit tests for each of your functions, something which is generally more tricky for object-oriented programming.","title":"Design patterns"},{"location":"guidelines/#programming","text":"","title":"Programming"},{"location":"guidelines/#mandatory","text":"NB: eventually these checks will be automatic Don't compare boolean values to True or False . Favour is not condition over not condition is Don't compare a value to None ( value == None ), always favour value is None","title":"Mandatory"},{"location":"guidelines/#encouraged","text":"Favour logging over print Favour using configuration files, or (faster/lazier/less good/ok) GLOBAL_VARIABLES near the top of your code, rather than repeated use of hard-coded variables in your code, particularly when with URL and file path variables like s3://path/to/some/fixed/place , but also for repeated numeric hyperparameters.","title":"Encouraged"},{"location":"guidelines/#critical-thinking","text":"The following questions should be going through your mind when developing your code. If you don't quite understand the wording or intention of the following questions then we encourage you to ask in the dev Slack channel! Surely this simple problem already has an elegant solution? How many copies of the data am I making in memory? Where do my variables go out of scope? How can I avoid creating a new variable at all costs? (think iterator, scope, lru_cache ) Have I made sure that I only run expensive or time consuming processes as few times as possible?","title":"Critical thinking"},{"location":"guidelines/#naming-conventions","text":"","title":"Naming conventions"},{"location":"guidelines/#mandatory_1","text":"Functions / methods: function , my_function (snake case) Variables / attributes: variable , my_var (snake case) Class: Model , MyClass (camel case) Module / file names / directory names: module , file_name.py , dir_name (camel case) Global* / constants: A_GLOBAL_VARIABLE (screaming snake case) * here we use \"Global\" to mean constants in scope at the module level, not the global level. Don't use global , ever.","title":"Mandatory"},{"location":"guidelines/#encouraged_1","text":"Keep all names as short and descriptive as possible. Variable names such as x or df are highly discouraged unless they are genuinely representing abstract concepts. Favour good naming conventions over helpful comments","title":"Encouraged"},{"location":"guidelines/#spaces-and-spacing","text":"NB: that using the autoformatter black in your IDE will resolve almost all of the following","title":"Spaces and spacing"},{"location":"guidelines/#encouraged_2","text":"Use the absolute minimum number of indents of your code. You can achieve this by writing modular code, and inverting logic, for example: def something ( args ): for item_collection in args : # 1 tab if item_collection . exists (): # 2 tabs the_sum = 0 # 3 tabs for item in item_collection : the_sum += item . value () # 4 tabs print ( the_sum ) can become: def sum_values ( item_collection ): the_sum = sum ( item . value () for item in item_collection ) print ( the_sum ) def something ( args ): for item_collection in args : # 1 tab if not item_collection . exists (): # 2 tabs continue # 3 tabs, buts unindents following lines sum_values ( item_collection ) # 2 tabs Put a space before starting block comments # like this , #not this Inline comments need two spaces before them a = 2 # like this Keep lines to 88 (officially 79) characters or less. You can achieve this by utilising other parts of this guideline, particularly with regards to creating modular code. Splitting over multiple lines is, of course, permissible so long as it doesn't conflict with legibility. When declaring default values, never put spaces around operators like = , i.e def this_is_ok(param=1) , def this_is_NOT_ok(param = 1) . Otherwise, all operators must always have a single space on either side. Separate function and class arguments with a comma and a space, i.e. do_thing(1, b=2) and not do_thing(1,b=2) .","title":"Encouraged"},{"location":"guidelines/#comments-and-docs","text":"","title":"Comments and docs"},{"location":"guidelines/#mandatory_2","text":"At least a basic docstring is required for every function/method and class Full, explanatory docstrings are required for all function/methods and classes if it will be used in the main body of a code routine. Use Google-style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html If you are using type hints then then you can write e.g. my_arg (str): description as simply my_arg: description .","title":"Mandatory"},{"location":"guidelines/#encouraged_3","text":"Don't state the obvious in comments Before writing a comment, consider whether that information would be better be encoded in a useful variable or function name.","title":"Encouraged"},{"location":"guidelines/#workflow","text":"This builds on a much greater body of work, laid out in nestauk/github_support . For avoidance of doubt, branches must be linked to a GitHub issue and named accordingly: { GitHub issue number } _ { tinyLowerCamelDescription } For example 14_readme , which indicates that this branch refered to this issue . You should generally favour having a dev branch, in addition to your main ( master ) branch. Never commit to dev , master or main . Only pull requests from branches named {GitHub issue number}_{tinyLowerCamelDescription} should ever be accepted. Please make all PRs and issues reasonably small: they should be trying to achieve roughly one task. Inevitably some simple tasks spawn large numbers of utils, and sometimes these detract from the original PR. In this case, you should stack an new PR on top of your \"base\" PR, for example as {GitHub issue number}_{differentLowerCamelDescription} . In this case the PR / Git merging tree will look like: dev <-- 123_originalThing <-- 423_differentThing <-- 578_anotherDifferentThing We can then merge the PR 123_originalThing into dev , then 423_differentThing into dev (after calling git merge dev on 423_differentThing ), etc until the chain is merged entirely. The nominated reviewer should review the entire chain, before the merge can go ahead. PRs should only be merged if all tests and a review has been signed off.","title":"Workflow"},{"location":"quickstart/","text":"Quickstart \u00b6 Requirements \u00b6 Python 3.6+ A *NIX system (e.g. Linux/macOS) - Windows might work, but we don't support it Mac users should also install: homebrew GNU coreutils - brew install coreutils . Conda Cookiecutter Python package >= 1.4.0: pip install cookiecutter # might be pip3 on your machine git-crypt - required for metaflow on AWS brew install git-crypt # mac apt-get install -y git-crypt # Ubuntu linux: github CLI [ OPTIONAL - required if auto_config set to true (the default)] - for automatic creation and configuration of a Github repo Install Mac: brew install gh Linux: https://github.com/cli/cli/blob/trunk/docs/install_linux.md Configure : gh auth login and answer prompts as below: What account do you want to log into? Github.com What is your preferred protocol for Git operations? SSH Upload your SSH public key to your Github account? Select the key you used to sign-up for 2-factor authentication with github How would you like to authenticate GitHub CLI? Login with a web browser Have a Nesta AWS account configured with awscli How do I do this? Fetch (or generate) security credentials from the AWS dashboard by clicking \"Create access key\". (In an environment with awscli installed via. pip/conda) Run aws configure , inputting the access key ID and secret access key ID you just generated when prompted. In addition you should set the default region name to eu-west-2 and the default output format to None . Reference: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config We recommend taking the time to install and configure the optional dependencies as this one-time setup allows you to use the make init command which saves a lot of time and avoids human error during configuration. Starting from scratch \u00b6 Create \u00b6 Ensure you have installed the requirements and then run cookiecutter https://github.com/nestauk/ds-cookiecutter . This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name : Type the name of your project here repo_name : Type the name of your repository here The default is a processed version of project_name that is compatible with python package names author_name : The author of the project description : A short description of your project openness : Whether the project is \"public\" (default) or \"private\" You should only make a project private if you have a good reason If \"public\" then an MIT license will be generated; otherwise the license will be a copyright placeholder If you choose auto_config as \"true\" then the Github repo created will obey this setting s3_bucket : The name of an S3 bucket to store assets for this project If you choose auto_config as \"true\" then this bucket will be created for you Careful : This needs to not conflict with any existing s3 bucket name and S3 bucket names must only contain lowercase letters, numbers, dots, and hyphens. This value can be reconfigured in .env.shared github_account : The github account that this project will be created in auto_config : Whether to automatically create a conda environment; github repo; S3 bucket; and configure AWS with metaflow. Requires optional requirement gh (the Github cli) to have been installed and configured (with gh auth login ) Configure \u00b6 If you selected auto_config as \"true\", the following actions have happened: A conda environment, project_name , has been created (with the project package installed in editable mode) Git pre-commit hooks have been configured and installed The Nesta metaflow config has been fetched and decrypted. It should exist in ~/.metaflowconfig/config.json A github repo github.com/nestauk/project_name has been created and configured An s3 bucket project_name has been created If you selected auto_config as \"false\", you will need to do the above manually (or run make init ): Run make install to configure the development environment: Setup the conda environment Configure pre-commit Configure metaflow to use AWS Manually create an S3 bucket s3_bucket (or run bash bin/create_bucket.sh ) Manually create a github repository (or run bash bin/create_repo.sh ) When you change directory to your created project folder, you will see that you are in a git branch 0_setup_cookiecutter . Make any tweaks to the cookiecutter required by your project ( see FAQ ), commit, and then make a Pull Request to dev . Collaborating on an existing project \u00b6 Clone the repository and cd into the repository. Run make install to configure the development environment: Setup the conda environment Install dependencies and local package Configure pre-commit Configure metaflow to use AWS conda activate project_name Check the project's README for any additional configuration needed (e.g. putting API keys in .env ) Pull any required inputs into inputs/ by running make inputs-pull Follow the author's documentation, or make them write some if they haven't already! How can I use this if I don't work at Nesta? \u00b6 You can still use the cookiecutter, you just need to remove a few Nesta-specific bits relating to our AWS setup: Choosing auto_config as \"false\" when setting up the cookiecutter Remove the setup-metaflow dependency of the install command within Makefile If using make init - replace references to nestauk with your github organisation in bin/create_repo.sh","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#requirements","text":"Python 3.6+ A *NIX system (e.g. Linux/macOS) - Windows might work, but we don't support it Mac users should also install: homebrew GNU coreutils - brew install coreutils . Conda Cookiecutter Python package >= 1.4.0: pip install cookiecutter # might be pip3 on your machine git-crypt - required for metaflow on AWS brew install git-crypt # mac apt-get install -y git-crypt # Ubuntu linux: github CLI [ OPTIONAL - required if auto_config set to true (the default)] - for automatic creation and configuration of a Github repo Install Mac: brew install gh Linux: https://github.com/cli/cli/blob/trunk/docs/install_linux.md Configure : gh auth login and answer prompts as below: What account do you want to log into? Github.com What is your preferred protocol for Git operations? SSH Upload your SSH public key to your Github account? Select the key you used to sign-up for 2-factor authentication with github How would you like to authenticate GitHub CLI? Login with a web browser Have a Nesta AWS account configured with awscli How do I do this? Fetch (or generate) security credentials from the AWS dashboard by clicking \"Create access key\". (In an environment with awscli installed via. pip/conda) Run aws configure , inputting the access key ID and secret access key ID you just generated when prompted. In addition you should set the default region name to eu-west-2 and the default output format to None . Reference: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config We recommend taking the time to install and configure the optional dependencies as this one-time setup allows you to use the make init command which saves a lot of time and avoids human error during configuration.","title":"Requirements"},{"location":"quickstart/#starting-from-scratch","text":"","title":"Starting from scratch"},{"location":"quickstart/#create","text":"Ensure you have installed the requirements and then run cookiecutter https://github.com/nestauk/ds-cookiecutter . This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name : Type the name of your project here repo_name : Type the name of your repository here The default is a processed version of project_name that is compatible with python package names author_name : The author of the project description : A short description of your project openness : Whether the project is \"public\" (default) or \"private\" You should only make a project private if you have a good reason If \"public\" then an MIT license will be generated; otherwise the license will be a copyright placeholder If you choose auto_config as \"true\" then the Github repo created will obey this setting s3_bucket : The name of an S3 bucket to store assets for this project If you choose auto_config as \"true\" then this bucket will be created for you Careful : This needs to not conflict with any existing s3 bucket name and S3 bucket names must only contain lowercase letters, numbers, dots, and hyphens. This value can be reconfigured in .env.shared github_account : The github account that this project will be created in auto_config : Whether to automatically create a conda environment; github repo; S3 bucket; and configure AWS with metaflow. Requires optional requirement gh (the Github cli) to have been installed and configured (with gh auth login )","title":"Create"},{"location":"quickstart/#configure","text":"If you selected auto_config as \"true\", the following actions have happened: A conda environment, project_name , has been created (with the project package installed in editable mode) Git pre-commit hooks have been configured and installed The Nesta metaflow config has been fetched and decrypted. It should exist in ~/.metaflowconfig/config.json A github repo github.com/nestauk/project_name has been created and configured An s3 bucket project_name has been created If you selected auto_config as \"false\", you will need to do the above manually (or run make init ): Run make install to configure the development environment: Setup the conda environment Configure pre-commit Configure metaflow to use AWS Manually create an S3 bucket s3_bucket (or run bash bin/create_bucket.sh ) Manually create a github repository (or run bash bin/create_repo.sh ) When you change directory to your created project folder, you will see that you are in a git branch 0_setup_cookiecutter . Make any tweaks to the cookiecutter required by your project ( see FAQ ), commit, and then make a Pull Request to dev .","title":"Configure"},{"location":"quickstart/#collaborating-on-an-existing-project","text":"Clone the repository and cd into the repository. Run make install to configure the development environment: Setup the conda environment Install dependencies and local package Configure pre-commit Configure metaflow to use AWS conda activate project_name Check the project's README for any additional configuration needed (e.g. putting API keys in .env ) Pull any required inputs into inputs/ by running make inputs-pull Follow the author's documentation, or make them write some if they haven't already!","title":"Collaborating on an existing project"},{"location":"quickstart/#how-can-i-use-this-if-i-dont-work-at-nesta","text":"You can still use the cookiecutter, you just need to remove a few Nesta-specific bits relating to our AWS setup: Choosing auto_config as \"false\" when setting up the cookiecutter Remove the setup-metaflow dependency of the install command within Makefile If using make init - replace references to nestauk with your github organisation in bin/create_repo.sh","title":"How can I use this if I don't work at Nesta?"},{"location":"roadmap/","text":"Roadmap \u00b6 Metaflow \u00b6 In the coming weeks we will be rolling out utilities to make working with metaflow easier. For example, you will be able to specify the following YAML file and then run a command like nestaflow sic_classifier and the corresponding metaflow pipeline will run on AWS batch with 8 CPU's, 64GB RAM with the configuration parameters specified in flow_kwargs . #file: src/config/pipeline/sic_classifier.yaml preflow_kwargs : with : batch,cpu=8,memory=64000 flow_kwargs : documents_path : inputs/data/descriptions.json freeze-model : false config : encode : add_special_tokens : true max_length : 64 pad_to_max_length : true Furthermore, successful runs will be tracked in version control allow data getters you write for metaflows to automatically fetch the right flow results. ds-utils \u00b6 Development of ds-utils will begin soon. This will be a package providing well tested and documented data science utilities based on our previous data-science projects. Reporting \u00b6 In a few recent projects we have been experimenting with a report workflow using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. After further refinement of this workflow and development of a simple report generation tool, this can be incorporated into the cookiecutter. flake8 \u00b6 Identification of a suitable starting flake8 configuration and a plan to phase in the number of cases handled by flake8 (to avoid an overwhelming start). Incorporation of flake8 into the pre-commit hooks. Note: you can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint . Poetry \u00b6 Investigate a switch to managing dependencies and packaging with poetry (falling back on Conda only when necessary). This will provide much more robust dependency management and a simplification of the packaging utilities. This tool is not being integrated into the overhauled cookiecutter from day one to avoid user overwhelm. Docker \u00b6 Investigate the utility of automatically containerisation data-science project. Schema \u00b6 Investigate the use of pydantic to document data-schemas and provide data-validation where required. This has the benefit of ensuring documentation stays up to date, and provides the ability to generate schemas in alternative forms such as converting to SQLalchemy and outputting in language-agnostic formats such as JSON schema . CI/CD \u00b6 Use Github actions to: - Automatically build a Docker container of the project - Automatically run tests - Perform pre-commit actions on the server to guard against user error Configuration \u00b6 Investigate options for Machine-learning oriented configuration management - e.g. with gin or the approach used by thinc .","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#metaflow","text":"In the coming weeks we will be rolling out utilities to make working with metaflow easier. For example, you will be able to specify the following YAML file and then run a command like nestaflow sic_classifier and the corresponding metaflow pipeline will run on AWS batch with 8 CPU's, 64GB RAM with the configuration parameters specified in flow_kwargs . #file: src/config/pipeline/sic_classifier.yaml preflow_kwargs : with : batch,cpu=8,memory=64000 flow_kwargs : documents_path : inputs/data/descriptions.json freeze-model : false config : encode : add_special_tokens : true max_length : 64 pad_to_max_length : true Furthermore, successful runs will be tracked in version control allow data getters you write for metaflows to automatically fetch the right flow results.","title":"Metaflow"},{"location":"roadmap/#ds-utils","text":"Development of ds-utils will begin soon. This will be a package providing well tested and documented data science utilities based on our previous data-science projects.","title":"ds-utils"},{"location":"roadmap/#reporting","text":"In a few recent projects we have been experimenting with a report workflow using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. After further refinement of this workflow and development of a simple report generation tool, this can be incorporated into the cookiecutter.","title":"Reporting"},{"location":"roadmap/#flake8","text":"Identification of a suitable starting flake8 configuration and a plan to phase in the number of cases handled by flake8 (to avoid an overwhelming start). Incorporation of flake8 into the pre-commit hooks. Note: you can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint .","title":"flake8"},{"location":"roadmap/#poetry","text":"Investigate a switch to managing dependencies and packaging with poetry (falling back on Conda only when necessary). This will provide much more robust dependency management and a simplification of the packaging utilities. This tool is not being integrated into the overhauled cookiecutter from day one to avoid user overwhelm.","title":"Poetry"},{"location":"roadmap/#docker","text":"Investigate the utility of automatically containerisation data-science project.","title":"Docker"},{"location":"roadmap/#schema","text":"Investigate the use of pydantic to document data-schemas and provide data-validation where required. This has the benefit of ensuring documentation stays up to date, and provides the ability to generate schemas in alternative forms such as converting to SQLalchemy and outputting in language-agnostic formats such as JSON schema .","title":"Schema"},{"location":"roadmap/#cicd","text":"Use Github actions to: - Automatically build a Docker container of the project - Automatically run tests - Perform pre-commit actions on the server to guard against user error","title":"CI/CD"},{"location":"roadmap/#configuration","text":"Investigate options for Machine-learning oriented configuration management - e.g. with gin or the approach used by thinc .","title":"Configuration"},{"location":"structure/","text":"Structure \u00b6 This page lays out where things belong according to high-level concepts. A direct tree representation of the folder hierarchy is also available. Example structures will soon be available to help you structure the lower-level folders which the cookiecutter leaves to you. In the following sections I use src/ to denote the project name to avoid awkward <project_name> placeholders. Project configuration - Makefile \u00b6 We use make to manage tasks relating to project setup/configuration/recurring tasks. make is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms. Running make from the project base directory will document the commands available along with a short description. Available rules: clean Delete all compiled Python files conda-create Create a conda environment conda-update Update the conda-environment based on changes to `environment.yaml` docs Build the API documentation docs-clean Clean the built API documentation docs-open Open the docs in the browser init Fully initialise a project: install; setup github repo; setup S3 bucket inputs-pull Pull `inputs/` from S3 inputs-push Push `inputs/` to S3 (WARNING: this may overwrite existing files!) install Install a project: create conda env; install local package; setup git hooks; setup metaflow+AWS lint Run flake8 linting on repository pip-install Install our package and requirements in editable mode (including development dependencies) pre-commit Perform pre-commit actions Where appropriate these make commands will automatically be run in the conda environment for a project. Git hooks \u00b6 We use pre-commit to check the integrity of git commits before they happen. The steps are specified in .pre-commit-config.yaml . Currently the steps that are taken are: Run the black code autoformatter This provides a consistent code style across a project and minimises messy git diffs (sometimes the code formatted by black may look \"uglier\" in places but this is the price we pay for having an industry standard with minimal cognitive burden) Check that no large files were accidentally committed Check that there are no merge conflict strings (e.g. >>>>> ) lingering in files Fix the end of files to work across operating systems Trim trailing whitespace in files Check Toml files are well formed Check Yaml files are well formed Check we are no committing directly to dev , master , or main Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML) Warning: You need to run git commit with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: pre-commit install --install-hooks will install the pre-commit hooks in the currently active environment). In time we will be integrating flake8 into these pre-commit hooks. You can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint . Reproducable environment \u00b6 The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis, but what tool should we use to do that? Whilst popular for scientific computing and data-science, conda poses problems for collaboration and packaging: It is hard to reproduce a conda-environment across operating systems It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda Files \u00b6 Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through pip install (one prominent example of this is graph-tool ). environment.yaml - Defines the base conda environment and any dependencies not \"pip-installable\". requirements.txt - Defines the dependences required to run the code. If you need to add a dependency, chances are it goes here! requirements_dev.txt - Defines development dependencies. These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and ipykernel to run code in jupyter (It's likely that you never need to think about this file) Commands \u00b6 make conda-create - Create a conda environment from environment.yaml and run make pip-install . Note: this is automatically called by make install and make init but exists as a stand-alone command in case you ever need it make conda-update - Update an existing conda environment from environment.yaml and run make pip-install . make pip-install - Install our package and requirements in editable mode (including development dependencies). Roadmap \u00b6 See roadmap for plans on improving packaging and reproducibility with Poetry and Docker . Secrets and configuration - .env.* and src/config/* \u00b6 You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Store your secrets in a special file \u00b6 Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname OTHER_VARIABLE=something You can use python-dotenv to load the entries as follows: import os from dotenv import load_dotenv , find_dotenv load_dotenv ( find_dotenv ()) # Find .env and load entries print ( os . getenv ( \"DATABASE_URL\" )) print ( os . getenv ( \"SOME_VARIABLE_NOT_IN_ENV_FILE\" )) # >> postgres://username:password@localhost:5432/dbname # >> None We also have .env.shared which contains non-secret project configuration variables that are used for example by commands in our Makefile Store Data-science configuration in src/config/ \u00b6 Few things scupper colloborative analysis like hard-coding hyper-parameters parameters deep in the code-base. src/config/base.yaml provides a place to document choices made. For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following, patstat_companies_house : match_threshold : 90 and load that value into your code with, from src import config config [ \"patstat_companies_house\" ][ \"match_threshold\" ] This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase. Aside - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. /home/alex/GIT/my_fantastic_data_project/outputs/data/foo.json , this will never work on anything other than your machine. Instead use relative paths and make use of src.PROJECT_DIR which will return the path to your project's base directory. This means you could specify the above path as f\"{src.PROJECT_DIR}/outputs/data/foo.json\" and have it work on everyone's machine! Roadmap \u00b6 See the roadmap for how src/config will be used to parameterise metaflow pipelines and version control their outputs. Data - inputs/data , outputs/data , outputs/.cache \u00b6 Firstly, don't version control data (inputs or outputs) in git, generally you should use s3 (directly or through metaflow) to manage your data. inputs/data \u00b6 Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial). Don't ever edit this raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. Store it in AWS S3 . When the project was configured, you will have been prompted for a BUCKET variable (now tracked in .env.shared ). If you used the auto_config option, an S3 bucket will have been setup for you too. Two make commands - make inputs-pull and make inputs-push - can be used to push and pull data from the configured s3 bucket. outputs/.cache/ \u00b6 This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents. For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in outputs/data ; or to temporary data when prototyping. outputs/data \u00b6 This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project. Fetching/loading data - src/getters \u00b6 This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks). This means that peppering calls like pd.read_csv(\"path/to/file\", sep=\"\\t\", ...) throughout the codebase should be strictly avoided. Following this approach means: If the format of path/to/file changes then we only have to make the change in one place We avoid inconsistencies such as forgetting to read a column in as a str instead of an int and thus missing leading zeroes. If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to pd.read_csv like we had above # File: getters/companies_house.py \"\"\"Data getters for the companies house data. Data source: https://download.companieshouse.gov.uk/en_output.html \"\"\" import pandas as pd def get_sector () -> pd . DataFrame : \"\"\"Load Companies House sector labels. Returns: Sector information for ... \"\"\" return pd . read_csv ( \"path/to/file\" , sep = \" \\t \" , dtype = { \"sic_code\" : str }) Roadmap \u00b6 On the roadmap is a speculative plan to explore the use of pydantic to specify and validate data-schemas. Pipeline components - src/pipeline \u00b6 This folder contains pipeline components. Put as much data-science as possible here. We recommend the use of metaflow to write these pipeline components. In the coming months as we roll out utilities and documentation to smooth out some of the rough edges of metaflow , this will become less of a recommendation and more of a stipulation. Using metaflow: Gives us lightweight version control of data and models Gives us easy access to AWS batch computing (including GPU machines) Makes it easy to take data-science code into production Shared utilities - src/utils \u00b6 This is a place to put utility functions needed across different parts of the codebase. For example, this could be functions shared across different pieces of analysis or different pipelines. Roadmap \u00b6 Over time there should be a decreasing need to add things to utils as we begin to develop a data science utilities package ( ds-utils ) . Analysis - src/analysis \u00b6 Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports. It is easier to say when shomething shouldn't be in analysis than when something should: If one part in analysis depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of analysis should be completely independent). It is important that plots are persisted to disk (in outputs/figures ). Notebooks \u00b6 Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules. Where does the humble notebook live? \u00b6 Notebooks should be placed as close to the place where their functionality will eventually reside as possible. For example, if you are prototyping a \"sentence transformer\" pipeline then that pipeline component will likely end up somewhere like pipeline/sentence_transformer/ , therefore you should place the notebooks for prototyping this features in pipeline/sentence_transformer/notebooks/ . If you're just getting started with a project and don't have a clear sense of the separation between analysis , pipeline , and getters yet (or it's too premature to split functionality across multiple places) then a sensible place to start is analysis/<high-level-description>/notebooks/ . Version control \u00b6 Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), we use jupytext to pair .ipynb files with a human-readable and git-diffable .py file. These paired .py files should be committed to git, .ipynb files are git-ignored. To ensure jupytext works correctly you should start jupyter (notebook/lab) from the base directory of the project so that jupyter detects the jupytext configuration that lives in jupytext.toml . Refactoring \u00b6 Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); where as others prefer to keep everything in notebooks until the last minute. We do not require you to work one way or the other as long as by the time you submit a pull request (PR) for your feature everything is refactored into python modules. Having said this, we recommended you frequently refactor the good parts - you'll thank yourself later! A warning sign you've left it too late to refactor is if you've got duplicates of functions across the codebase rather than importing from a logical place - if it's a data preprocessing task, put it in the pipeline at src/pipelines/<descriptive name for task> ; if it's useful utility code, refactor it to src/utils/ ; if it's loading data, refactor it to src/getters . Tips \u00b6 Add the following to your notebook (or IPython REPL): %load_ext autoreload %autoreload 2 Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc. When it comes to refactoring, open the python file Jupytext pairs to your notebook in your editor of choice - now your notebook code is easily-readable and in the same environment you use to write python modules. Share with gists \u00b6 As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token . Don't install jupyter / jupyterlab in your environment, use ipykernel \u00b6 You should avoid jupyter / jupyterlab as a dependency in the project environment. Instead add ipykernel as a dependency. This is a lightweight dependency that allows jupyter / jupyterlab installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project. Run python -m ipykernel install --user --name=<project environment name> from the environment where you run jupyter to allow jupyter to use your project's virtual environment. The advantages of this are: You only have to configure jupyter / jupyterlab once You will save disk-space Faster install Colleagues using other editors don't have to install heavy dependencies they don't use (you wouldn't be happy if someone sent you code that depended on VScode/pycharm/spyder) Note: ipykernel is also listed in requirements_dev.txt so you do not need to add it. Report - outputs/reports \u00b6 We are currently evaluating how we report data-science work - both at the project-level and the feature-level. Minimally, you should write reports in markdown putting them in outputs/reports and referencing plots in outputs/figures . We are experimenting with a toolchain using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. Tree \u00b6 \u251c\u2500\u2500 bin | PROJECT CONFIGURATION SCRIPTS \u2502 \u251c\u2500\u2500 conda_activate.sh | Helper to activate conda in shell environment \u2502 \u251c\u2500\u2500 create_bucket.sh | Create S3 bucket \u2502 \u251c\u2500\u2500 create_repo.sh | Create Github repo \u2502 \u2514\u2500\u2500 install_metaflow_aws.sh | Configure Metaflow with AWS \u251c\u2500\u2500 src | PYTHON PACKAGE \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 analysis | Analysis \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 config | Configuration \u2502 \u2502 \u251c\u2500\u2500 logging.yaml | logging configuration \u2502 \u2502 \u251c\u2500\u2500 base.yaml | global configuration (e.g. for tracking hyper-parameters) \u2502 \u2502 \u2514\u2500\u2500 pipeline | pipeline configuration files \u2502 \u2502 \u2514\u2500\u2500 .gitkeep | \u2502 \u251c\u2500\u2500 getters | Data getters \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 pipeline | Pipeline components \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u2514\u2500\u2500 utils | Utilities \u2502 \u2514\u2500\u2500 __init__.py | \u251c\u2500\u2500 docs | DOCUMENTATION \u2502 \u251c\u2500\u2500 conf.py | Configures docs \u2502 \u251c\u2500\u2500 index.rst | \u2502 \u2514\u2500\u2500 license.rst | \u251c\u2500\u2500 environment.yaml | CONDA ENVIRONMENT SPECIFICATION (optional component) \u251c\u2500\u2500 requirements.txt | PYTHON DEPENDENCIES NEEDED TO RUN THE CODE \u251c\u2500\u2500 requirements_dev.txt | PYTHON DEV DEPENDENCIES (e.g. building docs/running tests) \u251c\u2500\u2500 inputs | INPUTS (should be immutable) \u2502 \u251c\u2500\u2500 data | Inputs that are data \u2502 \u251c\u2500\u2500 models | Inputs that are models \u2502 \u2514\u2500\u2500 README.md | \u251c\u2500\u2500 jupytext.toml | JUPYTEXT CONFIGURATION \u251c\u2500\u2500 LICENSE | \u251c\u2500\u2500 outputs | OUTPUTS PRODUCED FROM THE PROJECT \u2502 \u251c\u2500\u2500 data | Data outputs (from running our code) \u2502 \u251c\u2500\u2500 figures | Figure outputs (from running our code) \u2502 \u2502 \u2514\u2500\u2500 vegalite | JSON specification of altair/vegalite figures \u2502 \u251c\u2500\u2500 models | Model outputs (from running our code) \u2502 \u2514\u2500\u2500 reports | Reports about our code and the results of running it \u251c\u2500\u2500 Makefile | TASKS TO COORDINATE PROJECT (`make` shows available commands) \u251c\u2500\u2500 README.md | \u251c\u2500\u2500 setup.py | ALLOWS US TO PIP INSTALL src/ \u251c\u2500\u2500 setup.cfg | ADDITIONAL PROJECT CONFIGURATION, e.g. linting \u251c\u2500\u2500 .pre-commit-config.yaml | DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS \u251c\u2500\u2500 .gitignore | TELLS git WHAT FILES WE DON'T WANT TO COMMIT \u251c\u2500\u2500 .github | GITHUB CONFIGURATION \u2502 \u2514\u2500\u2500 pull_request_template.md | Template for pull-requests (check-list of things to do) \u251c\u2500\u2500 .env | SECRETS (never commit to git!) \u251c\u2500\u2500 .env.shared | SHARED PROJECT CONFIGURATION VARIABLES","title":"Structure"},{"location":"structure/#structure","text":"This page lays out where things belong according to high-level concepts. A direct tree representation of the folder hierarchy is also available. Example structures will soon be available to help you structure the lower-level folders which the cookiecutter leaves to you. In the following sections I use src/ to denote the project name to avoid awkward <project_name> placeholders.","title":"Structure"},{"location":"structure/#project-configuration-makefile","text":"We use make to manage tasks relating to project setup/configuration/recurring tasks. make is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms. Running make from the project base directory will document the commands available along with a short description. Available rules: clean Delete all compiled Python files conda-create Create a conda environment conda-update Update the conda-environment based on changes to `environment.yaml` docs Build the API documentation docs-clean Clean the built API documentation docs-open Open the docs in the browser init Fully initialise a project: install; setup github repo; setup S3 bucket inputs-pull Pull `inputs/` from S3 inputs-push Push `inputs/` to S3 (WARNING: this may overwrite existing files!) install Install a project: create conda env; install local package; setup git hooks; setup metaflow+AWS lint Run flake8 linting on repository pip-install Install our package and requirements in editable mode (including development dependencies) pre-commit Perform pre-commit actions Where appropriate these make commands will automatically be run in the conda environment for a project.","title":"Project configuration - Makefile"},{"location":"structure/#git-hooks","text":"We use pre-commit to check the integrity of git commits before they happen. The steps are specified in .pre-commit-config.yaml . Currently the steps that are taken are: Run the black code autoformatter This provides a consistent code style across a project and minimises messy git diffs (sometimes the code formatted by black may look \"uglier\" in places but this is the price we pay for having an industry standard with minimal cognitive burden) Check that no large files were accidentally committed Check that there are no merge conflict strings (e.g. >>>>> ) lingering in files Fix the end of files to work across operating systems Trim trailing whitespace in files Check Toml files are well formed Check Yaml files are well formed Check we are no committing directly to dev , master , or main Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML) Warning: You need to run git commit with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: pre-commit install --install-hooks will install the pre-commit hooks in the currently active environment). In time we will be integrating flake8 into these pre-commit hooks. You can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint .","title":"Git hooks"},{"location":"structure/#reproducable-environment","text":"The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis, but what tool should we use to do that? Whilst popular for scientific computing and data-science, conda poses problems for collaboration and packaging: It is hard to reproduce a conda-environment across operating systems It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda","title":"Reproducable environment"},{"location":"structure/#files","text":"Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through pip install (one prominent example of this is graph-tool ). environment.yaml - Defines the base conda environment and any dependencies not \"pip-installable\". requirements.txt - Defines the dependences required to run the code. If you need to add a dependency, chances are it goes here! requirements_dev.txt - Defines development dependencies. These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and ipykernel to run code in jupyter (It's likely that you never need to think about this file)","title":"Files"},{"location":"structure/#commands","text":"make conda-create - Create a conda environment from environment.yaml and run make pip-install . Note: this is automatically called by make install and make init but exists as a stand-alone command in case you ever need it make conda-update - Update an existing conda environment from environment.yaml and run make pip-install . make pip-install - Install our package and requirements in editable mode (including development dependencies).","title":"Commands"},{"location":"structure/#roadmap","text":"See roadmap for plans on improving packaging and reproducibility with Poetry and Docker .","title":"Roadmap"},{"location":"structure/#secrets-and-configuration-env-and-srcconfig","text":"You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point.","title":"Secrets and configuration - .env.* and src/config/*"},{"location":"structure/#store-your-secrets-in-a-special-file","text":"Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname OTHER_VARIABLE=something You can use python-dotenv to load the entries as follows: import os from dotenv import load_dotenv , find_dotenv load_dotenv ( find_dotenv ()) # Find .env and load entries print ( os . getenv ( \"DATABASE_URL\" )) print ( os . getenv ( \"SOME_VARIABLE_NOT_IN_ENV_FILE\" )) # >> postgres://username:password@localhost:5432/dbname # >> None We also have .env.shared which contains non-secret project configuration variables that are used for example by commands in our Makefile","title":"Store your secrets in a special file"},{"location":"structure/#store-data-science-configuration-in-srcconfig","text":"Few things scupper colloborative analysis like hard-coding hyper-parameters parameters deep in the code-base. src/config/base.yaml provides a place to document choices made. For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following, patstat_companies_house : match_threshold : 90 and load that value into your code with, from src import config config [ \"patstat_companies_house\" ][ \"match_threshold\" ] This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase. Aside - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. /home/alex/GIT/my_fantastic_data_project/outputs/data/foo.json , this will never work on anything other than your machine. Instead use relative paths and make use of src.PROJECT_DIR which will return the path to your project's base directory. This means you could specify the above path as f\"{src.PROJECT_DIR}/outputs/data/foo.json\" and have it work on everyone's machine!","title":"Store Data-science configuration in src/config/"},{"location":"structure/#roadmap_1","text":"See the roadmap for how src/config will be used to parameterise metaflow pipelines and version control their outputs.","title":"Roadmap"},{"location":"structure/#data-inputsdata-outputsdata-outputscache","text":"Firstly, don't version control data (inputs or outputs) in git, generally you should use s3 (directly or through metaflow) to manage your data.","title":"Data - inputs/data, outputs/data, outputs/.cache"},{"location":"structure/#inputsdata","text":"Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial). Don't ever edit this raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. Store it in AWS S3 . When the project was configured, you will have been prompted for a BUCKET variable (now tracked in .env.shared ). If you used the auto_config option, an S3 bucket will have been setup for you too. Two make commands - make inputs-pull and make inputs-push - can be used to push and pull data from the configured s3 bucket.","title":"inputs/data"},{"location":"structure/#outputscache","text":"This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents. For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in outputs/data ; or to temporary data when prototyping.","title":"outputs/.cache/"},{"location":"structure/#outputsdata","text":"This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project.","title":"outputs/data"},{"location":"structure/#fetchingloading-data-srcgetters","text":"This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks). This means that peppering calls like pd.read_csv(\"path/to/file\", sep=\"\\t\", ...) throughout the codebase should be strictly avoided. Following this approach means: If the format of path/to/file changes then we only have to make the change in one place We avoid inconsistencies such as forgetting to read a column in as a str instead of an int and thus missing leading zeroes. If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to pd.read_csv like we had above # File: getters/companies_house.py \"\"\"Data getters for the companies house data. Data source: https://download.companieshouse.gov.uk/en_output.html \"\"\" import pandas as pd def get_sector () -> pd . DataFrame : \"\"\"Load Companies House sector labels. Returns: Sector information for ... \"\"\" return pd . read_csv ( \"path/to/file\" , sep = \" \\t \" , dtype = { \"sic_code\" : str })","title":"Fetching/loading data - src/getters"},{"location":"structure/#roadmap_2","text":"On the roadmap is a speculative plan to explore the use of pydantic to specify and validate data-schemas.","title":"Roadmap"},{"location":"structure/#pipeline-components-srcpipeline","text":"This folder contains pipeline components. Put as much data-science as possible here. We recommend the use of metaflow to write these pipeline components. In the coming months as we roll out utilities and documentation to smooth out some of the rough edges of metaflow , this will become less of a recommendation and more of a stipulation. Using metaflow: Gives us lightweight version control of data and models Gives us easy access to AWS batch computing (including GPU machines) Makes it easy to take data-science code into production","title":"Pipeline components - src/pipeline"},{"location":"structure/#shared-utilities-srcutils","text":"This is a place to put utility functions needed across different parts of the codebase. For example, this could be functions shared across different pieces of analysis or different pipelines.","title":"Shared utilities - src/utils"},{"location":"structure/#roadmap_3","text":"Over time there should be a decreasing need to add things to utils as we begin to develop a data science utilities package ( ds-utils ) .","title":"Roadmap"},{"location":"structure/#analysis-srcanalysis","text":"Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports. It is easier to say when shomething shouldn't be in analysis than when something should: If one part in analysis depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of analysis should be completely independent). It is important that plots are persisted to disk (in outputs/figures ).","title":"Analysis - src/analysis"},{"location":"structure/#notebooks","text":"Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules.","title":"Notebooks"},{"location":"structure/#where-does-the-humble-notebook-live","text":"Notebooks should be placed as close to the place where their functionality will eventually reside as possible. For example, if you are prototyping a \"sentence transformer\" pipeline then that pipeline component will likely end up somewhere like pipeline/sentence_transformer/ , therefore you should place the notebooks for prototyping this features in pipeline/sentence_transformer/notebooks/ . If you're just getting started with a project and don't have a clear sense of the separation between analysis , pipeline , and getters yet (or it's too premature to split functionality across multiple places) then a sensible place to start is analysis/<high-level-description>/notebooks/ .","title":"Where does the humble notebook live?"},{"location":"structure/#version-control","text":"Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), we use jupytext to pair .ipynb files with a human-readable and git-diffable .py file. These paired .py files should be committed to git, .ipynb files are git-ignored. To ensure jupytext works correctly you should start jupyter (notebook/lab) from the base directory of the project so that jupyter detects the jupytext configuration that lives in jupytext.toml .","title":"Version control"},{"location":"structure/#refactoring","text":"Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); where as others prefer to keep everything in notebooks until the last minute. We do not require you to work one way or the other as long as by the time you submit a pull request (PR) for your feature everything is refactored into python modules. Having said this, we recommended you frequently refactor the good parts - you'll thank yourself later! A warning sign you've left it too late to refactor is if you've got duplicates of functions across the codebase rather than importing from a logical place - if it's a data preprocessing task, put it in the pipeline at src/pipelines/<descriptive name for task> ; if it's useful utility code, refactor it to src/utils/ ; if it's loading data, refactor it to src/getters .","title":"Refactoring"},{"location":"structure/#tips","text":"Add the following to your notebook (or IPython REPL): %load_ext autoreload %autoreload 2 Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc. When it comes to refactoring, open the python file Jupytext pairs to your notebook in your editor of choice - now your notebook code is easily-readable and in the same environment you use to write python modules.","title":"Tips"},{"location":"structure/#share-with-gists","text":"As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token .","title":"Share with gists"},{"location":"structure/#dont-install-jupyterjupyterlab-in-your-environment-use-ipykernel","text":"You should avoid jupyter / jupyterlab as a dependency in the project environment. Instead add ipykernel as a dependency. This is a lightweight dependency that allows jupyter / jupyterlab installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project. Run python -m ipykernel install --user --name=<project environment name> from the environment where you run jupyter to allow jupyter to use your project's virtual environment. The advantages of this are: You only have to configure jupyter / jupyterlab once You will save disk-space Faster install Colleagues using other editors don't have to install heavy dependencies they don't use (you wouldn't be happy if someone sent you code that depended on VScode/pycharm/spyder) Note: ipykernel is also listed in requirements_dev.txt so you do not need to add it.","title":"Don't install jupyter/jupyterlab in your environment, use ipykernel"},{"location":"structure/#report-outputsreports","text":"We are currently evaluating how we report data-science work - both at the project-level and the feature-level. Minimally, you should write reports in markdown putting them in outputs/reports and referencing plots in outputs/figures . We are experimenting with a toolchain using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs.","title":"Report - outputs/reports"},{"location":"structure/#tree","text":"\u251c\u2500\u2500 bin | PROJECT CONFIGURATION SCRIPTS \u2502 \u251c\u2500\u2500 conda_activate.sh | Helper to activate conda in shell environment \u2502 \u251c\u2500\u2500 create_bucket.sh | Create S3 bucket \u2502 \u251c\u2500\u2500 create_repo.sh | Create Github repo \u2502 \u2514\u2500\u2500 install_metaflow_aws.sh | Configure Metaflow with AWS \u251c\u2500\u2500 src | PYTHON PACKAGE \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 analysis | Analysis \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 config | Configuration \u2502 \u2502 \u251c\u2500\u2500 logging.yaml | logging configuration \u2502 \u2502 \u251c\u2500\u2500 base.yaml | global configuration (e.g. for tracking hyper-parameters) \u2502 \u2502 \u2514\u2500\u2500 pipeline | pipeline configuration files \u2502 \u2502 \u2514\u2500\u2500 .gitkeep | \u2502 \u251c\u2500\u2500 getters | Data getters \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 pipeline | Pipeline components \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u2514\u2500\u2500 utils | Utilities \u2502 \u2514\u2500\u2500 __init__.py | \u251c\u2500\u2500 docs | DOCUMENTATION \u2502 \u251c\u2500\u2500 conf.py | Configures docs \u2502 \u251c\u2500\u2500 index.rst | \u2502 \u2514\u2500\u2500 license.rst | \u251c\u2500\u2500 environment.yaml | CONDA ENVIRONMENT SPECIFICATION (optional component) \u251c\u2500\u2500 requirements.txt | PYTHON DEPENDENCIES NEEDED TO RUN THE CODE \u251c\u2500\u2500 requirements_dev.txt | PYTHON DEV DEPENDENCIES (e.g. building docs/running tests) \u251c\u2500\u2500 inputs | INPUTS (should be immutable) \u2502 \u251c\u2500\u2500 data | Inputs that are data \u2502 \u251c\u2500\u2500 models | Inputs that are models \u2502 \u2514\u2500\u2500 README.md | \u251c\u2500\u2500 jupytext.toml | JUPYTEXT CONFIGURATION \u251c\u2500\u2500 LICENSE | \u251c\u2500\u2500 outputs | OUTPUTS PRODUCED FROM THE PROJECT \u2502 \u251c\u2500\u2500 data | Data outputs (from running our code) \u2502 \u251c\u2500\u2500 figures | Figure outputs (from running our code) \u2502 \u2502 \u2514\u2500\u2500 vegalite | JSON specification of altair/vegalite figures \u2502 \u251c\u2500\u2500 models | Model outputs (from running our code) \u2502 \u2514\u2500\u2500 reports | Reports about our code and the results of running it \u251c\u2500\u2500 Makefile | TASKS TO COORDINATE PROJECT (`make` shows available commands) \u251c\u2500\u2500 README.md | \u251c\u2500\u2500 setup.py | ALLOWS US TO PIP INSTALL src/ \u251c\u2500\u2500 setup.cfg | ADDITIONAL PROJECT CONFIGURATION, e.g. linting \u251c\u2500\u2500 .pre-commit-config.yaml | DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS \u251c\u2500\u2500 .gitignore | TELLS git WHAT FILES WE DON'T WANT TO COMMIT \u251c\u2500\u2500 .github | GITHUB CONFIGURATION \u2502 \u2514\u2500\u2500 pull_request_template.md | Template for pull-requests (check-list of things to do) \u251c\u2500\u2500 .env | SECRETS (never commit to git!) \u251c\u2500\u2500 .env.shared | SHARED PROJECT CONFIGURATION VARIABLES","title":"Tree"},{"location":"examples/","text":"Example project structures \u00b6 Constructing an industrial taxonomy using business website descriptions","title":"Examples"},{"location":"examples/#example-project-structures","text":"Constructing an industrial taxonomy using business website descriptions","title":"Example project structures"},{"location":"examples/industrial_taxonomy/","text":"Constructing an industrial taxonomy using business website descriptions \u00b6 Example under construction This example project is (loosely) based on work ongoing in nestauk/industrial-taxonomy , and the structure below is based on lessons learned in this project. The project is split into four high-level tasks ( , , , ) which we walk through. Elements of in particular have been simplified to keep the emphasis on the project structure rather than the project itself. You can skip ahead to the project tree if you want a birds-eye view. Matching of Glass to Companies House \u00b6 Method By fuzzy-matching data about UK business websites to Companies House based on company names we obtain a link between the text on business websites (describing businesses' activities) and the SIC codes (official industry codes) of that company. This work was performed in a separate project with the results stored and versioned in S3 by Metaflow. This can easily be retrieved using the metaflow client API. getters/inputs/{glass,companies_house.py,glass_house.py} Fetch all the data via. metaflow's client API. analysis/eda Exploratory data analysis of these data-sources SIC classifier \u00b6 Method Using the matched \"glass-house\" dataset train a classifier to predict SIC codes (this is developed as a general industry classifier that is agnostic to the SIC taxonomy as it is used elsewhere in the project). We can then conduct a meta-analysis looking at SIC codes that are under-represented by the classifiers predictions on validation data when compared to their \"true\" label obtained from the \"glass-house\" matching. pipeline/industry_classifier/{log_reg_model.py,transformer_model.py} Two competing models (not specific to SIC taxonomy) config/pipeline/industry_classifier/sic/*.yaml Parameterisation of model flows. What is the extra sic folder doing in the filepath? There's an extra folder, sic , in the config/ path that isn't present in the pipeline/ path because this project uses the industry classifier models across different taxonomies. In this case, sic denotes the fact that we are applying it to the SIC taxonomy and gives us a namespace within the config/ directory to isolate multiple uses of the same flow. Whats with all the config/pipeline/**/*.yaml ? In this example structure, we have individual YAML files for each pipeline component. This will be the structure we recommend once we add metaflow utilities to the cookiecutter. Whilst you are free to mimic this structure now, it is currently more convenient to nest the config/pipeline/** structure within base.yaml as it is easily importable as a python dict - from src import config . Furthermore, because metaflows are run from the command line and we want to parameterise them with YAML from config/pipeline** , each flow.py file currently needs an accompanying run.py file to: - Load and parse the YAML config needed for the pipeline - Form the pipeline arguments into a command to run the metaflow on the command line - Run the metaflow from within run.py using Python's subprocess library. - Update a config file with the successful metaflow run ID (so that getters know which version of the data to fetch) This is a lot of leg-work and increases the surface-area for bugs. This is why the upcoming metaflow utilities will enable one to run a command like nestaflow industry_classifier/sic/transformer_model (based on the paths in config/pipeline/** ) and all the above will be taken care of automatically without needing to write an accompanying run.py script. getters/outputs/sic_classifier.py Load trained model, giving access to predict functionality Why separate inputs and outputs in getters/ ? Separating inputs and outputs in getters is useful when reading the code - it allows us to differentiate between what is produced in this project and what we depend on from elsewhere. This is less useful when writing code - the import from src.getters.inputs.glass import get_sector is very long. To provide a shorter import we can do the following: # File: src/getters/__init__.py from .inputs import glass # File: src/analysis/example.py from src.getters.glass import get_sector Your directory structure doesn't always have to reflect the user-API! Avoid import ing from src/pipeline/ If you find yourself importing functions from src/pipeline in src/analysis then that functionality likely belongs in src/utils . Furthermore, src/analysis should get results of src/pipeline via. functions in src/getters . One exception that may occasionally arise when working with metaflow is needing to import a flow object itself - e.g. to access a static method or class method it defines (metaflow doesn't permit storing functions as data artifacts). analysis/sic_classifier/ Analysis of industry classifier models applied to SIC taxonomy model_selection.py - Evaluate competing models and pick best sic_meta_analysis.py - Meta-analysis looking at SIC codes under-represented in predictions and for which the model is over/under confident (informs which parts of the SIC taxonomy could be improved) Hierarchical topic modelling \u00b6 Method By training a TopSBM hierarchical topic model on the business website descriptions we can use the topics and clusters generated by the model and combine them with the SIC code labels to generate measures of sector similarity (how similar are any two SIC codes based on their cluster membership probabilities) and sector homogeneity (how homogeneous are is the topic distribution of descriptions aggregated by SIC code). Pre-processing \u00b6 The first step is to process raw business website descriptions into clean, tokenised, n-grammed representation that can be passed to the topic model (this is re-used in ). pipeline/glass_description_ngrams/{flow,utils}.py Metaflow to run spacy entity recognition; convert to tokens; and generate n-grams based on co-occurrence flow.py and utils.py We have lots of flow.py and utils.py . Some might see this as bad because it's not super-informative; however as long as the parent folder has an informative name then it's good enough. config/glass_description_ngrams.yaml Parameterisation of the above metaflow. getters/outputs/glass_house.py Getter to fetch tokenised n-grams of business website descriptions. pipeline/glass_description_ngrams/notebooks/ Sanity-checking of output results. Not a part of analysis/ because not directly analysing/presenting these results. Modelling \u00b6 Now the topic model itself can be run. config/pipeline/topsbm.yaml No corresponding flow in pipeline! Imported from a different library (e.g. ds-utils ) and used here getters/outputs/topsbm.py Fetch fitted model instance containing our inferred topics and clusters analysis/topsbm/ model_metadata.py - Output summary table of model fit, other metadata such as topic hierarchy, top words etc. - sector_similarity.py - Pair-wise similarity of SIC codes calculated using topsbm model outputs - sector_homogeneity.py - Homogeneity of SIC codes calculated using topsbm model outputs - utils.py - Hang on... Why is section_*.py not a pipeline component? It would be equally valid to place these in pipeline/ (because they are computing transformations of data) but analysis/ is also fine (and possibly better) because: Nothing else depends on these (so we aren't forced to refactor outside analysis/ ) The transformations done by these scripts are relatively quick (therefore there's no need to refactor into a metaflow in pipeline/ to save others from having to recompute a long-running analysis) These scripts would need to exist anyway to visualise and summarise the results for reporting Hang on... doesn't utils.py imply shared functionality in analysis ? If we had a flatter analysis/ folder - e.g. everything in analysis/topsbm/ was moved into analysis/ - this would be unacceptable; however it's just about okay to have a short utils file here. If we weren't happy with this we could put it in utils/topsbm.py : - For : other pieces of analysis or pipeline components may need to use these functions in the future, now they can without refactoring - Against : in this case it's only one common function which we're pretty sure is only needed here and now lives further away from where it's used Build a data-driven taxonomy \u00b6 Method Identify relevant terms within business website descriptions Build a co-occurrence network of terms and prune network Decompose co-occurrence into communities Label companies with their communities to add a new level onto the existing SIC taxonomy Apply industry classifier to perform similar meta-analysis to that done in Keyword extraction \u00b6 Use keyword extraction methods to tag business descriptions with items from the UNSPSC (a products and services taxonomy). Given this is successful, co-occurrence networks of products and services can be used to build a taxonomy. If this is unsuccessful, fall back on the n-gramming pipeline produced in . inputs/data/UNSPSC_English_v230701.xlsx New dataset for this project provided by the supplier as an excel spreadsheet analysis/eda/unspsc.py Explore the UNSPSC dataset getters/inputs/unspsc.py Function to load UNSPSC data Note: It's structured and clean enough that we don't need to do any preprocessing on it pipeline/keyword_extraction/*.py Metaflows to extract keywords from text using various methods and filter based on prescence in UNSPSC config/pipeline/keyword_extraction/*.yaml Parameterise above pipelines analysis/keyword_extraction/notebooks/ None of the keyword extraction approaches worked out No need to refactor notebooks into script if materials are not produced for final reporting. Notebooks exploring results of keyword extraction methods and comparing their effectiveness. N-gramming pipeline \u00b6 getters/outputs/glass_house.py Use same getter as produced in preprocessing step of . Constructing a term co-occurrence network and generating communities \u00b6 pipeline/kw_cooccurrence_taxonomy/*.py Metaflow and utils to construct a co-occurrence network of terms; decompose into communities; and label companies with their community labels config/pipeline/kw_cooccurrence_taxonomy.yaml Parameterise above flow analysis/kw_cooccurrence_taxonomy/visualise_structure.py Visualise the structure of the new taxonomy (and it's dependent communities) Applying the industry classifier to the new taxonomy level \u00b6 config/pipeline/industry_classifier/kw_cooccurrence_taxonomy.yaml Apply the industry classifier to our new taxonomies labels analysis/kw_cooccurrence_taxonomy/industry_classifier.py Perform a meta-analysis for our new taxonomy as was done in for the SIC taxonomy Project tree \u00b6 folders only \u251c\u2500\u2500 inputs \u2502 \u2514\u2500\u2500 data \u2514\u2500\u2500 src \u251c\u2500\u2500 analysis \u2502 \u251c\u2500\u2500 eda \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 keyword_extraction \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 sic_classifier \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 topsbm \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 pipeline \u2502 \u2514\u2500\u2500 industry_classifier \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2514\u2500\u2500 sic \u251c\u2500\u2500 getters \u2502 \u251c\u2500\u2500 inputs \u2502 \u2514\u2500\u2500 outputs \u251c\u2500\u2500 pipeline \u2502 \u251c\u2500\u2500 glass_description_ngrams \u2502 \u251c\u2500\u2500 industry_classifier \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 keyword_extraction \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 sic_taxonomy_data \u2514\u2500\u2500 utils \u251c\u2500\u2500 altair \u2514\u2500\u2500 metaflow folders and files TODO","title":"Constructing an industrial taxonomy using business website descriptions"},{"location":"examples/industrial_taxonomy/#constructing-an-industrial-taxonomy-using-business-website-descriptions","text":"Example under construction This example project is (loosely) based on work ongoing in nestauk/industrial-taxonomy , and the structure below is based on lessons learned in this project. The project is split into four high-level tasks ( , , , ) which we walk through. Elements of in particular have been simplified to keep the emphasis on the project structure rather than the project itself. You can skip ahead to the project tree if you want a birds-eye view.","title":"Constructing an industrial taxonomy using business website descriptions"},{"location":"examples/industrial_taxonomy/#matching-of-glass-to-companies-house","text":"Method By fuzzy-matching data about UK business websites to Companies House based on company names we obtain a link between the text on business websites (describing businesses' activities) and the SIC codes (official industry codes) of that company. This work was performed in a separate project with the results stored and versioned in S3 by Metaflow. This can easily be retrieved using the metaflow client API. getters/inputs/{glass,companies_house.py,glass_house.py} Fetch all the data via. metaflow's client API. analysis/eda Exploratory data analysis of these data-sources","title":"Matching of Glass to Companies House"},{"location":"examples/industrial_taxonomy/#sic-classifier","text":"Method Using the matched \"glass-house\" dataset train a classifier to predict SIC codes (this is developed as a general industry classifier that is agnostic to the SIC taxonomy as it is used elsewhere in the project). We can then conduct a meta-analysis looking at SIC codes that are under-represented by the classifiers predictions on validation data when compared to their \"true\" label obtained from the \"glass-house\" matching. pipeline/industry_classifier/{log_reg_model.py,transformer_model.py} Two competing models (not specific to SIC taxonomy) config/pipeline/industry_classifier/sic/*.yaml Parameterisation of model flows. What is the extra sic folder doing in the filepath? There's an extra folder, sic , in the config/ path that isn't present in the pipeline/ path because this project uses the industry classifier models across different taxonomies. In this case, sic denotes the fact that we are applying it to the SIC taxonomy and gives us a namespace within the config/ directory to isolate multiple uses of the same flow. Whats with all the config/pipeline/**/*.yaml ? In this example structure, we have individual YAML files for each pipeline component. This will be the structure we recommend once we add metaflow utilities to the cookiecutter. Whilst you are free to mimic this structure now, it is currently more convenient to nest the config/pipeline/** structure within base.yaml as it is easily importable as a python dict - from src import config . Furthermore, because metaflows are run from the command line and we want to parameterise them with YAML from config/pipeline** , each flow.py file currently needs an accompanying run.py file to: - Load and parse the YAML config needed for the pipeline - Form the pipeline arguments into a command to run the metaflow on the command line - Run the metaflow from within run.py using Python's subprocess library. - Update a config file with the successful metaflow run ID (so that getters know which version of the data to fetch) This is a lot of leg-work and increases the surface-area for bugs. This is why the upcoming metaflow utilities will enable one to run a command like nestaflow industry_classifier/sic/transformer_model (based on the paths in config/pipeline/** ) and all the above will be taken care of automatically without needing to write an accompanying run.py script. getters/outputs/sic_classifier.py Load trained model, giving access to predict functionality Why separate inputs and outputs in getters/ ? Separating inputs and outputs in getters is useful when reading the code - it allows us to differentiate between what is produced in this project and what we depend on from elsewhere. This is less useful when writing code - the import from src.getters.inputs.glass import get_sector is very long. To provide a shorter import we can do the following: # File: src/getters/__init__.py from .inputs import glass # File: src/analysis/example.py from src.getters.glass import get_sector Your directory structure doesn't always have to reflect the user-API! Avoid import ing from src/pipeline/ If you find yourself importing functions from src/pipeline in src/analysis then that functionality likely belongs in src/utils . Furthermore, src/analysis should get results of src/pipeline via. functions in src/getters . One exception that may occasionally arise when working with metaflow is needing to import a flow object itself - e.g. to access a static method or class method it defines (metaflow doesn't permit storing functions as data artifacts). analysis/sic_classifier/ Analysis of industry classifier models applied to SIC taxonomy model_selection.py - Evaluate competing models and pick best sic_meta_analysis.py - Meta-analysis looking at SIC codes under-represented in predictions and for which the model is over/under confident (informs which parts of the SIC taxonomy could be improved)","title":"SIC classifier"},{"location":"examples/industrial_taxonomy/#hierarchical-topic-modelling","text":"Method By training a TopSBM hierarchical topic model on the business website descriptions we can use the topics and clusters generated by the model and combine them with the SIC code labels to generate measures of sector similarity (how similar are any two SIC codes based on their cluster membership probabilities) and sector homogeneity (how homogeneous are is the topic distribution of descriptions aggregated by SIC code).","title":"Hierarchical topic modelling"},{"location":"examples/industrial_taxonomy/#pre-processing","text":"The first step is to process raw business website descriptions into clean, tokenised, n-grammed representation that can be passed to the topic model (this is re-used in ). pipeline/glass_description_ngrams/{flow,utils}.py Metaflow to run spacy entity recognition; convert to tokens; and generate n-grams based on co-occurrence flow.py and utils.py We have lots of flow.py and utils.py . Some might see this as bad because it's not super-informative; however as long as the parent folder has an informative name then it's good enough. config/glass_description_ngrams.yaml Parameterisation of the above metaflow. getters/outputs/glass_house.py Getter to fetch tokenised n-grams of business website descriptions. pipeline/glass_description_ngrams/notebooks/ Sanity-checking of output results. Not a part of analysis/ because not directly analysing/presenting these results.","title":"Pre-processing"},{"location":"examples/industrial_taxonomy/#modelling","text":"Now the topic model itself can be run. config/pipeline/topsbm.yaml No corresponding flow in pipeline! Imported from a different library (e.g. ds-utils ) and used here getters/outputs/topsbm.py Fetch fitted model instance containing our inferred topics and clusters analysis/topsbm/ model_metadata.py - Output summary table of model fit, other metadata such as topic hierarchy, top words etc. - sector_similarity.py - Pair-wise similarity of SIC codes calculated using topsbm model outputs - sector_homogeneity.py - Homogeneity of SIC codes calculated using topsbm model outputs - utils.py - Hang on... Why is section_*.py not a pipeline component? It would be equally valid to place these in pipeline/ (because they are computing transformations of data) but analysis/ is also fine (and possibly better) because: Nothing else depends on these (so we aren't forced to refactor outside analysis/ ) The transformations done by these scripts are relatively quick (therefore there's no need to refactor into a metaflow in pipeline/ to save others from having to recompute a long-running analysis) These scripts would need to exist anyway to visualise and summarise the results for reporting Hang on... doesn't utils.py imply shared functionality in analysis ? If we had a flatter analysis/ folder - e.g. everything in analysis/topsbm/ was moved into analysis/ - this would be unacceptable; however it's just about okay to have a short utils file here. If we weren't happy with this we could put it in utils/topsbm.py : - For : other pieces of analysis or pipeline components may need to use these functions in the future, now they can without refactoring - Against : in this case it's only one common function which we're pretty sure is only needed here and now lives further away from where it's used","title":"Modelling"},{"location":"examples/industrial_taxonomy/#build-a-data-driven-taxonomy","text":"Method Identify relevant terms within business website descriptions Build a co-occurrence network of terms and prune network Decompose co-occurrence into communities Label companies with their communities to add a new level onto the existing SIC taxonomy Apply industry classifier to perform similar meta-analysis to that done in","title":"Build a data-driven taxonomy"},{"location":"examples/industrial_taxonomy/#keyword-extraction","text":"Use keyword extraction methods to tag business descriptions with items from the UNSPSC (a products and services taxonomy). Given this is successful, co-occurrence networks of products and services can be used to build a taxonomy. If this is unsuccessful, fall back on the n-gramming pipeline produced in . inputs/data/UNSPSC_English_v230701.xlsx New dataset for this project provided by the supplier as an excel spreadsheet analysis/eda/unspsc.py Explore the UNSPSC dataset getters/inputs/unspsc.py Function to load UNSPSC data Note: It's structured and clean enough that we don't need to do any preprocessing on it pipeline/keyword_extraction/*.py Metaflows to extract keywords from text using various methods and filter based on prescence in UNSPSC config/pipeline/keyword_extraction/*.yaml Parameterise above pipelines analysis/keyword_extraction/notebooks/ None of the keyword extraction approaches worked out No need to refactor notebooks into script if materials are not produced for final reporting. Notebooks exploring results of keyword extraction methods and comparing their effectiveness.","title":"Keyword extraction"},{"location":"examples/industrial_taxonomy/#n-gramming-pipeline","text":"getters/outputs/glass_house.py Use same getter as produced in preprocessing step of .","title":"N-gramming pipeline"},{"location":"examples/industrial_taxonomy/#constructing-a-term-co-occurrence-network-and-generating-communities","text":"pipeline/kw_cooccurrence_taxonomy/*.py Metaflow and utils to construct a co-occurrence network of terms; decompose into communities; and label companies with their community labels config/pipeline/kw_cooccurrence_taxonomy.yaml Parameterise above flow analysis/kw_cooccurrence_taxonomy/visualise_structure.py Visualise the structure of the new taxonomy (and it's dependent communities)","title":"Constructing a term co-occurrence network and generating communities"},{"location":"examples/industrial_taxonomy/#applying-the-industry-classifier-to-the-new-taxonomy-level","text":"config/pipeline/industry_classifier/kw_cooccurrence_taxonomy.yaml Apply the industry classifier to our new taxonomies labels analysis/kw_cooccurrence_taxonomy/industry_classifier.py Perform a meta-analysis for our new taxonomy as was done in for the SIC taxonomy","title":"Applying the industry classifier to the new taxonomy level"},{"location":"examples/industrial_taxonomy/#project-tree","text":"folders only \u251c\u2500\u2500 inputs \u2502 \u2514\u2500\u2500 data \u2514\u2500\u2500 src \u251c\u2500\u2500 analysis \u2502 \u251c\u2500\u2500 eda \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 keyword_extraction \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 sic_classifier \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 topsbm \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 pipeline \u2502 \u2514\u2500\u2500 industry_classifier \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2514\u2500\u2500 sic \u251c\u2500\u2500 getters \u2502 \u251c\u2500\u2500 inputs \u2502 \u2514\u2500\u2500 outputs \u251c\u2500\u2500 pipeline \u2502 \u251c\u2500\u2500 glass_description_ngrams \u2502 \u251c\u2500\u2500 industry_classifier \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 keyword_extraction \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u251c\u2500\u2500 kw_cooccurrence_taxonomy \u2502 \u2502 \u2514\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 sic_taxonomy_data \u2514\u2500\u2500 utils \u251c\u2500\u2500 altair \u2514\u2500\u2500 metaflow folders and files TODO","title":"Project tree"}]}