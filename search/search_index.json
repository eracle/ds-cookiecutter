{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cookiecutter Data Science @ Nesta A logical, reasonably standardized, project structure for reproducible and collaborative pre-production data science work. Disclaimers: The workflow and the documentation here of it are works in progress and may currently be incomplete or inconsistent in parts - please raise issues where you spot this is the case. The foundations of this document are heavily borrowed (large parts of it verbatim) from the great work here by the people at Drivendata . High-level aims Shouldn\u2019t get in the way of rapid prototyping of ideas for an individual Analysis of one user should be runnable and reproducable by another user without changes Minimal computation and data transfer when rerunning the pipeline after changes Long-term integrity of the code-base without intervention Version control for data, models, outputs/metrics Reduce time to productionise analysis Getting started While this template focuses on Python, the general project structure can be used with another language after removing the Python boilerplate in the repo such as the the files in the src ( note that src will actually be named the same as your repo name) folder, and the Sphinx documentation skeleton in docs ). Requirements Python 3.6+ cookiecutter Python package >= 1.4.0: pip install cookiecutter A *NIX system (e.g. Linux/OSX) is required to ensure full functionality. Starting a new project Starting a new project is as easy as running this command at the command line. No need to create a directory first, the cookiecutter will do it for you. cookiecutter https://github.com/nestauk/cookiecutter-data-science-nesta For getting started quickly, have a look at the quickstart or FAQ . Reproducing analysis for an existing project If the project structure has been adhered to and an appropriate Makefile entry made then reproducing the analysis should be a one-liner (assuming your .env contains everything it needs to - nothing by default). make all Structure \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 README.md <- An inventory of data-sources, including schemas (or links to schemas) \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u251c\u2500\u2500 aux <- Non-automatable human interventions, e.g. hand selected IDs to ignore \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 logging.yaml <- Logging config \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 model_config.yaml <- Model configuration parameters \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Notebooks at the top level should have a markdown \u2502 \u2502 header outlining the notebook and should avoid function calls in \u2502 \u2502 favour of factored out code. \u2502 \u2502 \u2502 \u251c\u2500\u2500 notebook_preamble.ipy \u2502 \u2502 \u2502 \u2514\u2500\u2500 dev <- Development notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `_` delimited description, e.g. \u2502 `01_jmg_eda.ipynb`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u251c\u2500\u2500 eda <- Generated exploratory data analysis reports \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 conda_environment.yaml <- A reproducable conda environment. \u2502 installable with `conda env create -f conda_environment.yaml` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) and importable \u2502 \u251c\u2500\u2500 src <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2502 \u2502 \u251c\u2500\u2500 fetch_data.py <- Script to fetch data into data/raw \u2502 \u2502 \u2502 \u251c\u2500\u2500 make_dataset.py<- Scripts to generate processed data \u2502 \u2502 \u2502 \u251c\u2500\u2500 transformers <- Methods that perform `transform` on a dataset but not `fit` \u2502 \u2502 \u2502 \u251c\u2500\u2500 estimators <- Methods that perform `fit` on a dataset \u2502 \u2502 \u2502 \u2514\u2500\u2500 visualisation <- Scripts to create exploratory and results oriented visualisations \u2502 \u2514\u2500\u2500 tox.ini <- tox file with settings for running tox; see tox.testrun.org Overview Analysis is a DAG Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already, you don't want to wait to rerun them every time. make is one of the simplest ways for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms. Following the make documentation , Makefile conventions , and portability guide will help ensure your Makefiles work effectively across systems. Here are some examples to get started . A number of data folks use make as their tool of choice, including Mike Bostock . There are other tools for managing DAGs that are written in Python instead of a DSL (e.g., Luigi which is used in Nesta's production system ) - though these generally impose more constraints which can slow down the prototyping stage. While make is good for managing quick, and simple steps rerunning a complicated DAG may take a large amount of time to perform when perhaps only the last stage has actually changed. Of course, one could only run the last stage but how do you know with certainty that no other long lived stage has changed since you last ran it? This is why we need data and model version control (see DVC ). Several generic make commands are made available within the cookiecutter for performing certain tasks. Build from the environment up The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. One effective approach to this is use conda . By listing all of your requirements in the repository (we include a conda_environment.yaml file) you can easily track the packages needed to recreate the analysis. Here is a good workflow: Add any required dependencies to conda_environment.yaml Run make create_environment to create an environment with the required dependencies Add dependencies to conda_environment.yaml as you go so that others can reproduce your environment If you have more complex requirements for recreating your environment, consider a virtual machine based approach such as Docker or Vagrant . Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need. Keep secrets and configuration out of version control You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Here's one way to do this: Store your secrets and config variables in a special file Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname AWS_ACCESS_KEY=myaccesskey AWS_SECRET_ACCESS_KEY=mysecretkey OTHER_VARIABLE=something Use a package to load these variables automatically. If you look at the stub script in src/data/make_dataset.py , it uses a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with os.environ.get . Here's an example snippet adapted from the python-dotenv documentation: # src/data/dotenv_example.py import os from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until it's found dotenv_path = find_dotenv() # load up the entries as environment variables load_dotenv(dotenv_path) database_url = os.environ.get(\"DATABASE_URL\") other_variable = os.environ.get(\"OTHER_VARIABLE\") AWS CLI configuration When using Amazon S3 to store data, a simple method of managing AWS access is to set your access keys to environment variables. However, managing mutiple sets of keys on a single machine (e.g. when working on multiple projects) it is best to use a credentials file , typically located in ~/.aws/credentials . A typical file might look like: [default] aws_access_key_id=myaccesskey aws_secret_access_key=mysecretkey [another_project] aws_access_key_id=myprojectaccesskey aws_secret_access_key=myprojectsecretkey You can add the profile name when initialising a project; assuming no applicable environment variables are set, the profile credentials will be used be default. Data folder data/raw Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG ), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw . src/fetch_data.py should call scripts/functions to fetch all the raw data you need. Also, if data is immutable, it doesn't need source control in the same way that code does. Raw data should be stored with s3 AWS S3 . Currently by default, we ask for an S3 bucket and use AWS CLI to sync data in the data/raw folder with the server. data/raw is in .gitignore by default. Two make commands - make sync_data_from_s3 and make sync_data_to_s3 - can be used to sync data to and from the configured s3 bucket ( BUCKET in Makefile ) data/interim This folder is for intermediate data that has been transformed but is not a fully processed output, perhaps as it is the output of a rough notebook. This is up to the individual user to manage as nothing important should live here. data/interim is in .gitignore by default. data/aux Some analyses may not be runnable from start to finish without human intervention, these \"hand-written\" datasets should be stored in data/aux and tracked in git. For example, clusters for unsupervised learning may need to be labelled or certain rows of a dataset dropped after manual inspection. For this reason it is important that analyses are exactly reproducable where possible as if clusters change then so will their label indices (see seeds and workers ) . data/processed This folder should contain transformed and processed data that is to be used in the final analysis or is a data output of the final analysis. data/processed is in .gitignore by default; however if you are using DVC you may want to remove it from .gitignore as anything here should be version controlled with DVC+S3 (see Data and model version control with DVC ). Data and model version control with DVC DVC is an open-source version control system for Machine Learning Projects designed to make models and data shareable and reproducible. Short and long tutorials are available within their documentation. An example tutorial specific to this cookiecutter is being developed (see Tutorial ). Brief overview DVC integrates with and works very similar to git, for example: dvc add images tracks a file just like git add dvc remote add myrepo s3://mybucket adds a remote just like git remote add dvc push pushes changes just like git push DVC also allows the expression of DAG's. dvc run -f cnn.dvc -d images -o model cnn.py generates a cnn.dvc file which contains MD5 hashes for each of the dependencies ( images ), outputs ( model ), and the file which is executed ( cnn.py ) along with the dependency information. The dependencies and outputs will be stored in the DVC cache, while cnn.dvc can be tracked by Git to link the given model output to the current commit. Running `dvc repro cnn.dvc** will reproduce this step, and if the current hashes exist in the DVC cache then no work will be done which may save re-running expensive training steps when sharing a repository. We are currently working on incorporating DVC into the cookiecutter Notebooks Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, they can pose challenges when reproducing an analysis and when trying to track changes with git. Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), collaborating directly with others on Jupyter notebooks should not be attempted. Naming Follow a naming convention that shows the owner and the order the analysis was done in. We use the format <step>_<initials>_<description>.ipynb (e.g., 01_jmg_eda.ipynb ). Refactoring Refactor the good parts (frequently). Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at src/make_dataset.py . If it's useful utility code, refactor it to src . Now by default we turn the project into a Python package (see the setup.py file). You can import your code and use it in notebooks with a cell like the following: from src.data import make_dataset Reducing boilerplate The first cell of most notebooks look mostly the same: a lot of library imports, a few IPython magic commands, and a few variable definitions. This can be simplified by factoring out the common parts into a notebooks/notebook_preamble.ipy file which can be run at the top of a notebook as follows: %run notebook_preamble.ipy This provides a few pieces of functionality: Sets matplotlib to inline Loads the autoreload extension and configures it such that changes from src will immediately be re-imported into the notebook Imports core python scientific stack (numpy/scipy/pandas/matplotlib) Defines project_dir and data_path variables to avoid hard-coded paths Sets up logging to log both within the notebook and to notebooks.log Git filter As mentioned above, source control with notebooks is tricky. One way to increase the readability of diff 's is to use a Git filter to strip notebooks of their output before performing any action with git. This is done using a short jq command. The git filter is automatically installed to .git/config and .gitattributes when starting a cookiecutter project, but anyone cloning/forking the repository will need to install the filter manually by running make git . Share with gists As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token . Git There are many different ways to use Git each with their pros and cons, refer to your organisations workflow (Nesta members can find ours in the playbook ). Style and Documentation PEP8 Basic PEP8 and style requirements apply. The make lint command can be used to use black to format code. Code formatting can be heavily opinion based with much debate over things such as linelengths etc. We recommend the use of black which gives fast, deterministic code-formatting that avoids wasting mental energy of worrying about code formatting and improves code review speed by formatting code in a way that produces smaller diffs. It is convenient to run from the command line or incorporate into your favourite editor . Doc-strings Google doc-strings should be written for every exposable function/class. In addition, it is proposed to add a #UTILS flag at the end of a docstring for functions/classes that should be put into a utilities package for re-use across projects and subject to more rigorous testing and standardisation. Data README data/README.md should document everything in data/{raw,aux,processed} . For example # raw ## something_from_online.xlsx <Details about what the file contains> Obtained via `wget` by running `make fetch` ### Columns <column name 0> : <brief description> ... <column name N> : <brief description> # aux ## drop_ids.txt Contains one id corresponding to <column name 0> in `raw/something_from_online.xlsx` per line which should be dropped due to formatting errors. # processed ## processed_dataset.csv Cleaned and processed version of `data/raw/something_from_online.xlsx`. Produced by running `make data` ### Columns <column name 0> : <brief description> ... <column name N> : <brief description> Logging logging module should be used over print statements. There are multiple reasons for this such as the ability to log severity levels, use timestamps, and know the origin of a log message. The Logging cookbook is a good resource to understand how the logging module works but the cookiecutter is setup such that logging is setup with a good default using both logging.yaml and src/__init__.py . In order to setup logging to both stdout / stderr and to info.log and errors.log all one needs to do is import the src repository and initalise a logger object: import <repo_name> import logging logger = logging.getLogger(__name__) logger.info('Logs at INFO level to info.log') logger.info('Logs at ERROR level to errors.log') No hard-coding Few things scupper colloborative analysis like hard-coding configuration parameters deep in the code-base. Where possible we believe that data/configuration and code should remain separate. Several different entities can end up being buried in code for which we provide a solution to avoiding. Paths Hard-coding paths to data - e.g. `\"/home/nesta/project/data/raw/file.csv\" - means that everytime someone wishes to run your analysis they have to find every path and correct it to their system. Furthermore, if there are several contributors to a repository then different users paths will cause a lot of unneccessary diff's at best and a lot of merge conflicts to resolve at worst. For this reason never hard-code a folder path. Use the <repo_name>.project_dir variable to refer to paths, e.g. f'{project_dir}/data/raw/file.csv' . For some analyses you may be using the same raw data across multiple projects and not have the disk space to store multiple copies in the respective data/raw folders. Rather than reference a hard-coded external directory (e.g. /storage/file.csv ), one should create a symlink from both data/raw folders pointing to the external directory such that f'{project_dir}/data/raw/file.csv' will actually reference /storage/file.csv without requiring the data to be stored multiple times. Parameters A data science project typically involves at least several dozen decisions on what thresholds, hyper-parameters etc. to use which if buried in the code-base will never be on the radar of someone who didn't write the code and is often forgotten even by the person who wrote it! Tracking these choices in model_config.yaml gives these high visibility and makes the analysis more transparent. YAML is easy to read, easy to write , and easy to load: import yaml fname = f'{project_dir}/model_config.yaml' with open(fname, 'r') as f: params = yaml.safe_load(f) For convenience, we've set it up so you can use import <repo_name>; <repo_name>.config to access your YAML config. Alternatively, you may wish to use your favourite configuration parser. Seeds and workers Pseudo-random number generation (PRNG) lies at the core of many of the bread and butter data science techniques, and it is therefore important to keep track of the seeds used for each analysis to ensure that results are as exactly reproducible as possible across runs. This can be done by setting the seeds for the PRNG's used at the start of any analysis by using numpy.random.seed(0) and random.seed(0) . Note: avoid calling this at multiple points throughout the code as re-seeding the PRNG will cause the same numbers to be drawn again in the same order. Certain algorithms such as Gensim's Word2Vec implementation use Python's built-in hash-function which is seeded by the environment variable PYTHONHASHSEED meaning that the environment variable needs to be set with export PYTHONHASHSEED=0 to ensure reproducibility. Finally, another consideration is the number of threads/cores to use for analysis and to make this easy configurable. Different users may have different computational resources available to them and may opt to run on more or fewer cores. Furthermore, this choice may affect reproducibility as many multi-threaded algorithms are not exactly reproducible due to OS thread scheduling therefore if exact reproducibility is necessary then only one worker should be used. Cookiecutter in practice A tutorial outline has been developed and will be collaboratively updated as we adopt this new way of working. The tutorial outlines how this project structure works in practice end-to-end for a simple analysis of the Gateway to Research data trying to predict which UK research council funded a research proposal based on its abstract. NOTE: This is currently incomplete and slightly out of date. Roadmap Add an option to configure the cookiecutter to make easier use of DVC data_getters.labs If the need to share an analysis adhering to this workflow across repositories arises, this generally suggests that it should probably be put into the production system; however this might not be possible to do quickly, or may be slightly premature. In this case, outputs to share should be pushed to the s3 nesta-data-getters and a thin wrapper function added to the labs subpackage of data_getters that fetches this data, and signposts to the original analysis. labs will be periodically reviewed to asssess whether components should be productionised or deprecated. Plans to outline and implement a procedure for factoring out #UTILS functions into nesta.packages Code review guidelines Testing A guide for how to write tests for Data science code will be developed and testing requirements will be phased which will require a given level of testing for all code, data, models that are used as a component of any analysis which is exposed beyond the developers team. Plotting style Consistent visual grammar of graphics to produce consistent high quality plots for various outputs (papers, blogs, reports, presentations etc.). EDA framework A guide to performing EDA on a new dataset and producing a summary report of the output - this would also assist in the data auditing process (see the Nesta blog ). Thanks Finally, a thanks to Drivendata and the Cookiecutter project ( github ).","title":"Cookiecutter Data Science @ Nesta"},{"location":"#cookiecutter-data-science-nesta","text":"A logical, reasonably standardized, project structure for reproducible and collaborative pre-production data science work.","title":"Cookiecutter Data Science @ Nesta"},{"location":"#disclaimers","text":"The workflow and the documentation here of it are works in progress and may currently be incomplete or inconsistent in parts - please raise issues where you spot this is the case. The foundations of this document are heavily borrowed (large parts of it verbatim) from the great work here by the people at Drivendata .","title":"Disclaimers:"},{"location":"#high-level-aims","text":"Shouldn\u2019t get in the way of rapid prototyping of ideas for an individual Analysis of one user should be runnable and reproducable by another user without changes Minimal computation and data transfer when rerunning the pipeline after changes Long-term integrity of the code-base without intervention Version control for data, models, outputs/metrics Reduce time to productionise analysis","title":"High-level aims"},{"location":"#getting-started","text":"While this template focuses on Python, the general project structure can be used with another language after removing the Python boilerplate in the repo such as the the files in the src ( note that src will actually be named the same as your repo name) folder, and the Sphinx documentation skeleton in docs ).","title":"Getting started"},{"location":"#requirements","text":"Python 3.6+ cookiecutter Python package >= 1.4.0: pip install cookiecutter A *NIX system (e.g. Linux/OSX) is required to ensure full functionality.","title":"Requirements"},{"location":"#starting-a-new-project","text":"Starting a new project is as easy as running this command at the command line. No need to create a directory first, the cookiecutter will do it for you. cookiecutter https://github.com/nestauk/cookiecutter-data-science-nesta For getting started quickly, have a look at the quickstart or FAQ .","title":"Starting a new project"},{"location":"#reproducing-analysis-for-an-existing-project","text":"If the project structure has been adhered to and an appropriate Makefile entry made then reproducing the analysis should be a one-liner (assuming your .env contains everything it needs to - nothing by default). make all","title":"Reproducing analysis for an existing project"},{"location":"#structure","text":"\u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 README.md <- An inventory of data-sources, including schemas (or links to schemas) \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u251c\u2500\u2500 aux <- Non-automatable human interventions, e.g. hand selected IDs to ignore \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 logging.yaml <- Logging config \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 model_config.yaml <- Model configuration parameters \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Notebooks at the top level should have a markdown \u2502 \u2502 header outlining the notebook and should avoid function calls in \u2502 \u2502 favour of factored out code. \u2502 \u2502 \u2502 \u251c\u2500\u2500 notebook_preamble.ipy \u2502 \u2502 \u2502 \u2514\u2500\u2500 dev <- Development notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `_` delimited description, e.g. \u2502 `01_jmg_eda.ipynb`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u251c\u2500\u2500 eda <- Generated exploratory data analysis reports \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 conda_environment.yaml <- A reproducable conda environment. \u2502 installable with `conda env create -f conda_environment.yaml` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) and importable \u2502 \u251c\u2500\u2500 src <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2502 \u2502 \u251c\u2500\u2500 fetch_data.py <- Script to fetch data into data/raw \u2502 \u2502 \u2502 \u251c\u2500\u2500 make_dataset.py<- Scripts to generate processed data \u2502 \u2502 \u2502 \u251c\u2500\u2500 transformers <- Methods that perform `transform` on a dataset but not `fit` \u2502 \u2502 \u2502 \u251c\u2500\u2500 estimators <- Methods that perform `fit` on a dataset \u2502 \u2502 \u2502 \u2514\u2500\u2500 visualisation <- Scripts to create exploratory and results oriented visualisations \u2502 \u2514\u2500\u2500 tox.ini <- tox file with settings for running tox; see tox.testrun.org","title":"Structure"},{"location":"#overview","text":"","title":"Overview"},{"location":"#analysis-is-a-dag","text":"Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already, you don't want to wait to rerun them every time. make is one of the simplest ways for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms. Following the make documentation , Makefile conventions , and portability guide will help ensure your Makefiles work effectively across systems. Here are some examples to get started . A number of data folks use make as their tool of choice, including Mike Bostock . There are other tools for managing DAGs that are written in Python instead of a DSL (e.g., Luigi which is used in Nesta's production system ) - though these generally impose more constraints which can slow down the prototyping stage. While make is good for managing quick, and simple steps rerunning a complicated DAG may take a large amount of time to perform when perhaps only the last stage has actually changed. Of course, one could only run the last stage but how do you know with certainty that no other long lived stage has changed since you last ran it? This is why we need data and model version control (see DVC ). Several generic make commands are made available within the cookiecutter for performing certain tasks.","title":"Analysis is a DAG"},{"location":"#build-from-the-environment-up","text":"The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. One effective approach to this is use conda . By listing all of your requirements in the repository (we include a conda_environment.yaml file) you can easily track the packages needed to recreate the analysis. Here is a good workflow: Add any required dependencies to conda_environment.yaml Run make create_environment to create an environment with the required dependencies Add dependencies to conda_environment.yaml as you go so that others can reproduce your environment If you have more complex requirements for recreating your environment, consider a virtual machine based approach such as Docker or Vagrant . Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need.","title":"Build from the environment up"},{"location":"#keep-secrets-and-configuration-out-of-version-control","text":"You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Here's one way to do this:","title":"Keep secrets and configuration out of version control"},{"location":"#store-your-secrets-and-config-variables-in-a-special-file","text":"Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname AWS_ACCESS_KEY=myaccesskey AWS_SECRET_ACCESS_KEY=mysecretkey OTHER_VARIABLE=something","title":"Store your secrets and config variables in a special file"},{"location":"#use-a-package-to-load-these-variables-automatically","text":"If you look at the stub script in src/data/make_dataset.py , it uses a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with os.environ.get . Here's an example snippet adapted from the python-dotenv documentation: # src/data/dotenv_example.py import os from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until it's found dotenv_path = find_dotenv() # load up the entries as environment variables load_dotenv(dotenv_path) database_url = os.environ.get(\"DATABASE_URL\") other_variable = os.environ.get(\"OTHER_VARIABLE\")","title":"Use a package to load these variables automatically."},{"location":"#aws-cli-configuration","text":"When using Amazon S3 to store data, a simple method of managing AWS access is to set your access keys to environment variables. However, managing mutiple sets of keys on a single machine (e.g. when working on multiple projects) it is best to use a credentials file , typically located in ~/.aws/credentials . A typical file might look like: [default] aws_access_key_id=myaccesskey aws_secret_access_key=mysecretkey [another_project] aws_access_key_id=myprojectaccesskey aws_secret_access_key=myprojectsecretkey You can add the profile name when initialising a project; assuming no applicable environment variables are set, the profile credentials will be used be default.","title":"AWS CLI configuration"},{"location":"#data-folder","text":"","title":"Data folder"},{"location":"#dataraw","text":"Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG ), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw . src/fetch_data.py should call scripts/functions to fetch all the raw data you need. Also, if data is immutable, it doesn't need source control in the same way that code does. Raw data should be stored with s3 AWS S3 . Currently by default, we ask for an S3 bucket and use AWS CLI to sync data in the data/raw folder with the server. data/raw is in .gitignore by default. Two make commands - make sync_data_from_s3 and make sync_data_to_s3 - can be used to sync data to and from the configured s3 bucket ( BUCKET in Makefile )","title":"data/raw"},{"location":"#datainterim","text":"This folder is for intermediate data that has been transformed but is not a fully processed output, perhaps as it is the output of a rough notebook. This is up to the individual user to manage as nothing important should live here. data/interim is in .gitignore by default.","title":"data/interim"},{"location":"#dataaux","text":"Some analyses may not be runnable from start to finish without human intervention, these \"hand-written\" datasets should be stored in data/aux and tracked in git. For example, clusters for unsupervised learning may need to be labelled or certain rows of a dataset dropped after manual inspection. For this reason it is important that analyses are exactly reproducable where possible as if clusters change then so will their label indices (see seeds and workers ) .","title":"data/aux"},{"location":"#dataprocessed","text":"This folder should contain transformed and processed data that is to be used in the final analysis or is a data output of the final analysis. data/processed is in .gitignore by default; however if you are using DVC you may want to remove it from .gitignore as anything here should be version controlled with DVC+S3 (see Data and model version control with DVC ).","title":"data/processed"},{"location":"#data-and-model-version-control-with-dvc","text":"DVC is an open-source version control system for Machine Learning Projects designed to make models and data shareable and reproducible. Short and long tutorials are available within their documentation. An example tutorial specific to this cookiecutter is being developed (see Tutorial ).","title":"Data and model version control with DVC"},{"location":"#brief-overview","text":"DVC integrates with and works very similar to git, for example: dvc add images tracks a file just like git add dvc remote add myrepo s3://mybucket adds a remote just like git remote add dvc push pushes changes just like git push DVC also allows the expression of DAG's. dvc run -f cnn.dvc -d images -o model cnn.py generates a cnn.dvc file which contains MD5 hashes for each of the dependencies ( images ), outputs ( model ), and the file which is executed ( cnn.py ) along with the dependency information. The dependencies and outputs will be stored in the DVC cache, while cnn.dvc can be tracked by Git to link the given model output to the current commit. Running `dvc repro cnn.dvc** will reproduce this step, and if the current hashes exist in the DVC cache then no work will be done which may save re-running expensive training steps when sharing a repository. We are currently working on incorporating DVC into the cookiecutter","title":"Brief overview"},{"location":"#notebooks","text":"Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, they can pose challenges when reproducing an analysis and when trying to track changes with git. Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), collaborating directly with others on Jupyter notebooks should not be attempted.","title":"Notebooks"},{"location":"#naming","text":"Follow a naming convention that shows the owner and the order the analysis was done in. We use the format <step>_<initials>_<description>.ipynb (e.g., 01_jmg_eda.ipynb ).","title":"Naming"},{"location":"#refactoring","text":"Refactor the good parts (frequently). Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at src/make_dataset.py . If it's useful utility code, refactor it to src . Now by default we turn the project into a Python package (see the setup.py file). You can import your code and use it in notebooks with a cell like the following: from src.data import make_dataset","title":"Refactoring"},{"location":"#reducing-boilerplate","text":"The first cell of most notebooks look mostly the same: a lot of library imports, a few IPython magic commands, and a few variable definitions. This can be simplified by factoring out the common parts into a notebooks/notebook_preamble.ipy file which can be run at the top of a notebook as follows: %run notebook_preamble.ipy This provides a few pieces of functionality: Sets matplotlib to inline Loads the autoreload extension and configures it such that changes from src will immediately be re-imported into the notebook Imports core python scientific stack (numpy/scipy/pandas/matplotlib) Defines project_dir and data_path variables to avoid hard-coded paths Sets up logging to log both within the notebook and to notebooks.log","title":"Reducing boilerplate"},{"location":"#git-filter","text":"As mentioned above, source control with notebooks is tricky. One way to increase the readability of diff 's is to use a Git filter to strip notebooks of their output before performing any action with git. This is done using a short jq command. The git filter is automatically installed to .git/config and .gitattributes when starting a cookiecutter project, but anyone cloning/forking the repository will need to install the filter manually by running make git .","title":"Git filter"},{"location":"#share-with-gists","text":"As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token .","title":"Share with gists"},{"location":"#git","text":"There are many different ways to use Git each with their pros and cons, refer to your organisations workflow (Nesta members can find ours in the playbook ).","title":"Git"},{"location":"#style-and-documentation","text":"","title":"Style and Documentation"},{"location":"#pep8","text":"Basic PEP8 and style requirements apply. The make lint command can be used to use black to format code. Code formatting can be heavily opinion based with much debate over things such as linelengths etc. We recommend the use of black which gives fast, deterministic code-formatting that avoids wasting mental energy of worrying about code formatting and improves code review speed by formatting code in a way that produces smaller diffs. It is convenient to run from the command line or incorporate into your favourite editor .","title":"PEP8"},{"location":"#doc-strings","text":"Google doc-strings should be written for every exposable function/class. In addition, it is proposed to add a #UTILS flag at the end of a docstring for functions/classes that should be put into a utilities package for re-use across projects and subject to more rigorous testing and standardisation.","title":"Doc-strings"},{"location":"#data-readme","text":"data/README.md should document everything in data/{raw,aux,processed} . For example # raw ## something_from_online.xlsx <Details about what the file contains> Obtained via `wget` by running `make fetch` ### Columns <column name 0> : <brief description> ... <column name N> : <brief description> # aux ## drop_ids.txt Contains one id corresponding to <column name 0> in `raw/something_from_online.xlsx` per line which should be dropped due to formatting errors. # processed ## processed_dataset.csv Cleaned and processed version of `data/raw/something_from_online.xlsx`. Produced by running `make data` ### Columns <column name 0> : <brief description> ... <column name N> : <brief description>","title":"Data README"},{"location":"#logging","text":"logging module should be used over print statements. There are multiple reasons for this such as the ability to log severity levels, use timestamps, and know the origin of a log message. The Logging cookbook is a good resource to understand how the logging module works but the cookiecutter is setup such that logging is setup with a good default using both logging.yaml and src/__init__.py . In order to setup logging to both stdout / stderr and to info.log and errors.log all one needs to do is import the src repository and initalise a logger object: import <repo_name> import logging logger = logging.getLogger(__name__) logger.info('Logs at INFO level to info.log') logger.info('Logs at ERROR level to errors.log')","title":"Logging"},{"location":"#no-hard-coding","text":"Few things scupper colloborative analysis like hard-coding configuration parameters deep in the code-base. Where possible we believe that data/configuration and code should remain separate. Several different entities can end up being buried in code for which we provide a solution to avoiding.","title":"No hard-coding"},{"location":"#paths","text":"Hard-coding paths to data - e.g. `\"/home/nesta/project/data/raw/file.csv\" - means that everytime someone wishes to run your analysis they have to find every path and correct it to their system. Furthermore, if there are several contributors to a repository then different users paths will cause a lot of unneccessary diff's at best and a lot of merge conflicts to resolve at worst. For this reason never hard-code a folder path. Use the <repo_name>.project_dir variable to refer to paths, e.g. f'{project_dir}/data/raw/file.csv' . For some analyses you may be using the same raw data across multiple projects and not have the disk space to store multiple copies in the respective data/raw folders. Rather than reference a hard-coded external directory (e.g. /storage/file.csv ), one should create a symlink from both data/raw folders pointing to the external directory such that f'{project_dir}/data/raw/file.csv' will actually reference /storage/file.csv without requiring the data to be stored multiple times.","title":"Paths"},{"location":"#parameters","text":"A data science project typically involves at least several dozen decisions on what thresholds, hyper-parameters etc. to use which if buried in the code-base will never be on the radar of someone who didn't write the code and is often forgotten even by the person who wrote it! Tracking these choices in model_config.yaml gives these high visibility and makes the analysis more transparent. YAML is easy to read, easy to write , and easy to load: import yaml fname = f'{project_dir}/model_config.yaml' with open(fname, 'r') as f: params = yaml.safe_load(f) For convenience, we've set it up so you can use import <repo_name>; <repo_name>.config to access your YAML config. Alternatively, you may wish to use your favourite configuration parser.","title":"Parameters"},{"location":"#seeds-and-workers","text":"Pseudo-random number generation (PRNG) lies at the core of many of the bread and butter data science techniques, and it is therefore important to keep track of the seeds used for each analysis to ensure that results are as exactly reproducible as possible across runs. This can be done by setting the seeds for the PRNG's used at the start of any analysis by using numpy.random.seed(0) and random.seed(0) . Note: avoid calling this at multiple points throughout the code as re-seeding the PRNG will cause the same numbers to be drawn again in the same order. Certain algorithms such as Gensim's Word2Vec implementation use Python's built-in hash-function which is seeded by the environment variable PYTHONHASHSEED meaning that the environment variable needs to be set with export PYTHONHASHSEED=0 to ensure reproducibility. Finally, another consideration is the number of threads/cores to use for analysis and to make this easy configurable. Different users may have different computational resources available to them and may opt to run on more or fewer cores. Furthermore, this choice may affect reproducibility as many multi-threaded algorithms are not exactly reproducible due to OS thread scheduling therefore if exact reproducibility is necessary then only one worker should be used.","title":"Seeds and workers"},{"location":"#cookiecutter-in-practice","text":"A tutorial outline has been developed and will be collaboratively updated as we adopt this new way of working. The tutorial outlines how this project structure works in practice end-to-end for a simple analysis of the Gateway to Research data trying to predict which UK research council funded a research proposal based on its abstract. NOTE: This is currently incomplete and slightly out of date.","title":"Cookiecutter in practice"},{"location":"#roadmap","text":"Add an option to configure the cookiecutter to make easier use of DVC data_getters.labs If the need to share an analysis adhering to this workflow across repositories arises, this generally suggests that it should probably be put into the production system; however this might not be possible to do quickly, or may be slightly premature. In this case, outputs to share should be pushed to the s3 nesta-data-getters and a thin wrapper function added to the labs subpackage of data_getters that fetches this data, and signposts to the original analysis. labs will be periodically reviewed to asssess whether components should be productionised or deprecated. Plans to outline and implement a procedure for factoring out #UTILS functions into nesta.packages Code review guidelines Testing A guide for how to write tests for Data science code will be developed and testing requirements will be phased which will require a given level of testing for all code, data, models that are used as a component of any analysis which is exposed beyond the developers team. Plotting style Consistent visual grammar of graphics to produce consistent high quality plots for various outputs (papers, blogs, reports, presentations etc.). EDA framework A guide to performing EDA on a new dataset and producing a summary report of the output - this would also assist in the data auditing process (see the Nesta blog ).","title":"Roadmap"},{"location":"#thanks","text":"Finally, a thanks to Drivendata and the Cookiecutter project ( github ).","title":"Thanks"},{"location":"faq/","text":"FAQ How do I update conda_environment.yaml ? As you install extra libraries add them to conda_environment.yaml . If you're not sure what libraries you have installed you can use conda env export to print out all the versions of your current environment; however this will list dependencies too so don't blindly add everything! If you require specific versions of libraries, try to make these as general as possible to increase the probabiliity of somebody using a different machine being able to reproduce your environment. For example, pandas >0.25,<0.26 not pandas=0.25.3=py36hb3f55d8_0 . I didn't configure an s3 bucket at start-up, how do I do it? Open Makefile and edit the right-hand side of BUCKET = ... line so that is the name of your s3 bucket (don't include s3:// ). How should I put data into data/raw Edit fetch_data.py in the source folder to fetch any data from the web (from S3/SQL/an API etc.). Running make fetch will run this then sync the data/raw folder to s3 Where should I save models? Save trained models in models/ as something like a pickle ( .pkl ) file. Where do I get data? Use make sync_data_from_s3 to get the data from data/raw (as long as whoever set up the repo has configured an s3 bucket and used make sync_data_to_s3 ). How do I format code? Running make lint will format code in the directory named after your repo using black . Black makes code review faster by producing the smallest diffs possible. Blackened code looks the same regardless of the project you\u2019re reading. Formatting becomes transparent after a while and you can focus on the content instead. Where do I save figures? reports/figures Where should I put \"X\" functionality? Common sense applies but generally: If it's specific to a dataset, create a subfolder based on the dataset It it's a generic estimator (does transform but not fit ) put it in estimators/ . Pre-trained models fetched from e.g. tensorflow hub, should go here too If it's a generic transfomers (does fit ) put it in transformers/ If it creates a visualisation, put it in visualisation/ What is a dev notebook? dev notebooks are for exploration and experimention. A guiding principle: If someone else has to look inside your dev notebook, something has gone wrong (see \"What shouldn't be in a notebook?\") What shouldn't be in a notebook? Notebooks should serve two purposes: 1) Exploration/experimentation: notebooks/dev 2) Presention: notebooks/ If you have a notebook containing code that creates an asset (fetches data/processes data/defines a model/trains a model/produces a final plot etc. ), this should be refactored into a .py file in the source directory (see \"Where should I put \"X\" functionality?\"). How should I use model_config.yaml Configurable hyper-parameters should go in here. There should be no magic numbers in function bodies. If you're sure a number will not change (maybe it represents a fixed API ratelimit), define it in the main body of the file with an all-caps name to signal it's constant nature (e.g. MAX_REQUESTS_PER_SEC = 30 ). If it's a hyper-parameter (e.g. the number of trees in a random forest, or a random seed), put it in model_config.yaml : gtr_data: random_forest: seed: 0 n_estimators: 100 Fetch the value in the relevant file like so: import <repo_name> ... seed = <repo_name>.config['gtr_data']['random_forest']['seed'] Depending on the repo size, different levels of nesting in the config may be appropriate (E.g. If you only have one model and one dataset, no nesting is fine) Docstrings Any exposed function or class should have a google style docstring. Functions for internal use only should have a leading underscore (e.g. _my_fun ), and do not require full docstrings. See https://www.sphinx-doc.org/en/1.7/ext/example_google.html for examples of how to format docstrings. Sharing notebooks The cookiecutter installs a git hook to strip notebooks of output before commiting. This gives cleaner diff's, and stops repo bloat. If you git clone a cookiecutter repo and plan on making commits you should run make git to install this git hook. Notebooks should be shared with collaborators outside of the main git repository, either exported as HTML or as a gist using something like gist-it .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-do-i-update-conda_environmentyaml","text":"As you install extra libraries add them to conda_environment.yaml . If you're not sure what libraries you have installed you can use conda env export to print out all the versions of your current environment; however this will list dependencies too so don't blindly add everything! If you require specific versions of libraries, try to make these as general as possible to increase the probabiliity of somebody using a different machine being able to reproduce your environment. For example, pandas >0.25,<0.26 not pandas=0.25.3=py36hb3f55d8_0 .","title":"How do I update conda_environment.yaml?"},{"location":"faq/#i-didnt-configure-an-s3-bucket-at-start-up-how-do-i-do-it","text":"Open Makefile and edit the right-hand side of BUCKET = ... line so that is the name of your s3 bucket (don't include s3:// ).","title":"I didn't configure an s3 bucket at start-up, how do I do it?"},{"location":"faq/#how-should-i-put-data-into-dataraw","text":"Edit fetch_data.py in the source folder to fetch any data from the web (from S3/SQL/an API etc.). Running make fetch will run this then sync the data/raw folder to s3","title":"How should I put data into data/raw"},{"location":"faq/#where-should-i-save-models","text":"Save trained models in models/ as something like a pickle ( .pkl ) file.","title":"Where should I save models?"},{"location":"faq/#where-do-i-get-data","text":"Use make sync_data_from_s3 to get the data from data/raw (as long as whoever set up the repo has configured an s3 bucket and used make sync_data_to_s3 ).","title":"Where do I get data?"},{"location":"faq/#how-do-i-format-code","text":"Running make lint will format code in the directory named after your repo using black . Black makes code review faster by producing the smallest diffs possible. Blackened code looks the same regardless of the project you\u2019re reading. Formatting becomes transparent after a while and you can focus on the content instead.","title":"How do I format code?"},{"location":"faq/#where-do-i-save-figures","text":"reports/figures","title":"Where do I save figures?"},{"location":"faq/#where-should-i-put-x-functionality","text":"Common sense applies but generally: If it's specific to a dataset, create a subfolder based on the dataset It it's a generic estimator (does transform but not fit ) put it in estimators/ . Pre-trained models fetched from e.g. tensorflow hub, should go here too If it's a generic transfomers (does fit ) put it in transformers/ If it creates a visualisation, put it in visualisation/","title":"Where should I put \"X\" functionality?"},{"location":"faq/#what-is-a-dev-notebook","text":"dev notebooks are for exploration and experimention. A guiding principle: If someone else has to look inside your dev notebook, something has gone wrong (see \"What shouldn't be in a notebook?\")","title":"What is a dev notebook?"},{"location":"faq/#what-shouldnt-be-in-a-notebook","text":"Notebooks should serve two purposes: 1) Exploration/experimentation: notebooks/dev 2) Presention: notebooks/ If you have a notebook containing code that creates an asset (fetches data/processes data/defines a model/trains a model/produces a final plot etc. ), this should be refactored into a .py file in the source directory (see \"Where should I put \"X\" functionality?\").","title":"What shouldn't be in a notebook?"},{"location":"faq/#how-should-i-use-model_configyaml","text":"Configurable hyper-parameters should go in here. There should be no magic numbers in function bodies. If you're sure a number will not change (maybe it represents a fixed API ratelimit), define it in the main body of the file with an all-caps name to signal it's constant nature (e.g. MAX_REQUESTS_PER_SEC = 30 ). If it's a hyper-parameter (e.g. the number of trees in a random forest, or a random seed), put it in model_config.yaml : gtr_data: random_forest: seed: 0 n_estimators: 100 Fetch the value in the relevant file like so: import <repo_name> ... seed = <repo_name>.config['gtr_data']['random_forest']['seed'] Depending on the repo size, different levels of nesting in the config may be appropriate (E.g. If you only have one model and one dataset, no nesting is fine)","title":"How should I use model_config.yaml"},{"location":"faq/#docstrings","text":"Any exposed function or class should have a google style docstring. Functions for internal use only should have a leading underscore (e.g. _my_fun ), and do not require full docstrings. See https://www.sphinx-doc.org/en/1.7/ext/example_google.html for examples of how to format docstrings.","title":"Docstrings"},{"location":"faq/#sharing-notebooks","text":"The cookiecutter installs a git hook to strip notebooks of output before commiting. This gives cleaner diff's, and stops repo bloat. If you git clone a cookiecutter repo and plan on making commits you should run make git to install this git hook. Notebooks should be shared with collaborators outside of the main git repository, either exported as HTML or as a gist using something like gist-it .","title":"Sharing notebooks"},{"location":"quickstart/","text":"Quickstart Starting from scratch Create cookiecutter pip install cookiecutter cookiecutter https://github.com/nestauk/cookiecutter-data-science-nesta This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name [project_name]: Type the name of your project here repo_name [project_name]: Type the name of your repository here author_name [Nesta]: The author of the project (you or your organisation) description [...]: A short description of your project Select open_source_license: ... Choose the license you wish to use s3_bucket ...: The name of an S3 bucket to sync your raw data to aws_profile [default]: The AWS profile name to use for syncing to S3. Choose default unless your an advanced user. cd <repo_name> Create a virtual environment Add any libraries you know you will need into conda_environment.yaml Run make create_environment - This will setup a conda environment named according to your repository, and install your project as a local, editable package conda activate <environment name> ( conda env list will list available environments if you're unsure what your environment is called) First step We've setup our project structure and environment, now you're ready to get going! The best first step is to write <repo_name>/fetch_data.py to fetch all your data, and store it in data/raw/ . make fetch will run this file and sync data/raw to s3 for you (as long as you setup a bucket). Reproducing someone else's work Clone the repository and cd into the repository. Run make create_environment - This will setup a conda environment named according to your repository, and install your project as a local, editable package. conda activate <environment name> will activate the environment ( conda env list will list available environments if you're unsure what your environment is called) Check the README for any configuration needed (e.g. putting API keys in .env ) Get the raw data: make sync_from_s3 Follow their documentation, or make them write some if they haven't already!","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#starting-from-scratch","text":"","title":"Starting from scratch"},{"location":"quickstart/#create-cookiecutter","text":"pip install cookiecutter cookiecutter https://github.com/nestauk/cookiecutter-data-science-nesta This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name [project_name]: Type the name of your project here repo_name [project_name]: Type the name of your repository here author_name [Nesta]: The author of the project (you or your organisation) description [...]: A short description of your project Select open_source_license: ... Choose the license you wish to use s3_bucket ...: The name of an S3 bucket to sync your raw data to aws_profile [default]: The AWS profile name to use for syncing to S3. Choose default unless your an advanced user. cd <repo_name>","title":"Create cookiecutter"},{"location":"quickstart/#create-a-virtual-environment","text":"Add any libraries you know you will need into conda_environment.yaml Run make create_environment - This will setup a conda environment named according to your repository, and install your project as a local, editable package conda activate <environment name> ( conda env list will list available environments if you're unsure what your environment is called)","title":"Create a virtual environment"},{"location":"quickstart/#first-step","text":"We've setup our project structure and environment, now you're ready to get going! The best first step is to write <repo_name>/fetch_data.py to fetch all your data, and store it in data/raw/ . make fetch will run this file and sync data/raw to s3 for you (as long as you setup a bucket).","title":"First step"},{"location":"quickstart/#reproducing-someone-elses-work","text":"Clone the repository and cd into the repository. Run make create_environment - This will setup a conda environment named according to your repository, and install your project as a local, editable package. conda activate <environment name> will activate the environment ( conda env list will list available environments if you're unsure what your environment is called) Check the README for any configuration needed (e.g. putting API keys in .env ) Get the raw data: make sync_from_s3 Follow their documentation, or make them write some if they haven't already!","title":"Reproducing someone else's work"},{"location":"tutorial/","text":"NOTE TO NON-NESTA VISITORS : This tutorial is incomplete Introduction In this tutorial we are going to cover the workflow for creating a supervised model. In this instance, we will train a classifier to predict the name of the body who funded a research project from Gateway to Research based on its abstract text. The main aims of this tutorial are to learn how to structure the repository, prototype, and factor out code into modules so that the model and any processed data can be reproduced by anyone else with a simple command. In this tutorial, we will: Collect raw data from MySQL Explore the data in a notebook Write a data README Clean and preprocess the data Factor out cleaning and preprocessing code into modules Prototype feature extraction and model in a notebook Train and save the modelling pipeline Factor out feature extraction and modelling code into modules Put model and data under version control Update model Along the way, we should also keep a lab book, to document what we\u2019ve been doing. The Repository To begin with, clone the repository into a directory on your machine with the git clone command: $ git clone https://github.com/nestauk/baking-cookies.git $ # OR: git clone git@github.com:nestauk/baking-cookies.git Move in to the repository directory: $ cd baking-cookies If you type git branch , you can see that you are on the master branch. In general, we want to maintain the master branch for completely finished work, so we will work from the dev branch instead (within the settings for a Github repository you can change the default branch from master if you wish). $ git checkout dev From dev we will create two more branches. One will be for the sub-project - creating our model - and another will be your personal branch for work in progress (WIP). We make this second one because it is possible that both you and someone else will be prototyping for this sub-project at the same time, and it will be better if you have separate spaces to do this. For example, this allows you to backup your work by pushing to the remote without worrying about someone else pulling in those changes. Do not use someone else's personal branch : personal branches are liable to be deleted/rebased/make breaking changes. Use the -b flag to create and checkout your new branches simultaneously. I\u2019m going to use my initials (\"gr\") for the second branch, but you can use your own if you prefer. $ git checkout -b gtr-predict-funder $ git checkout -b gtr-predict-funder-gr-wip On your personal branch, you can prototype in notebooks that aren\u2019t intended for mass readership (though you may still want to make them readable enough to show collaborators). This is where we will work until we\u2019re ready to write more polished code and produce models. While we\u2019re getting set up, we should install the repository package in to the Python environment you want to use. $ pip install -e . We recommend (disk space permitting) that you have one environment per project. You can initialise a new conda environment by running make create_environment , and install the environment requirements in conda_environment.yaml with make requirements . When adding requirements to conda_environment.yaml only add the major dependencies (e.g. pandas not everything it depends on), and do not restrict the version numbers too much (e.g. - pandas=0.24.2 rather than - pandas=0.24.2=py37he6710b0_0 ) - this will avoid difficulties with other people on different operating systems not being able to reproduce your environment. Loading, Cleaning and Preprocessing You might have noticed that there is a notebooks folder in the project root directory. In here, there is a subdirectory, dev . This folder is for any notebooks that can be considered work in progress. See our guide to using notebooks . Launch jupyter and create a notebook in notebooks/dev/ for cleaning and processing the data. With our naming convention in mind, we will call this notebook 01_gr_gtr_projects . Loading The data we need for this tutorial is on Nesta\u2019s MySQL database. From the screenshot below you can see how we will write some simple code to grab the table we need, but there are also a few important things to note: The first cell in the notebook magic runs the preamble script. This loads some useful data science libraries and runs some useful boilerplate code so that we can quickly access variables such as the project directory. You can adapt this script to your project\u2019s needs. We hard code config_path to point to the location of our database config file. This is fine for now because we are working in a notebook that no one else is expected to be able to run seamlessly, but we will change it later. After collecting the data, we show the head of the dataframe. Printing and showing outputs at stages along the prototyping should be standard practice so that if you do show someone else the notebook, they can easily see what\u2019s going on, without needing to run the code. After loading and inspecting the data we will add another cell that saves it locally as a flat file to our raw directory. projects.to_csv(f'{data_path}/raw/gtr_projects.csv', index=False) EDA All new data should be explored before it is worked with in any serious capacity. NOTE : The EDA in this tutorial is not very rigorous. More rigorous guidelines for EDA are on the roadmap . This section of the tutorial will be updated accordingly in the future. Here we will use pandas_profiling and some more specific exploration. If you don\u2019t have it you can easily install it with $ pip install pandas-profiling The profiler produces a standard output, and covers many data quality elements. It\u2019s likely to be useful for anyone else looking at this dataset, so we will also save it in the reports directory as a html file. Now we can dive in to the specific features that we\u2019re interested in. First we look at the distribution of funders. It looks like we have fairly good representation from all of the funders, with over 5,000 projects for each of them apart from NC3Rs. Let\u2019s also look at the distribution of abstract text lengths. There\u2019s a peak at around 4,000 characters which presumably relates to a length limitation imposed on submitted abstracts. However, there\u2019s also a peak close to 0. Let\u2019s take a look at the distributions by funder and also have a closer look at that low end. It looks like there\u2019s some projects with very few characters which we will probably discard. However, there are a high frequency of abstracts with lengths at around 250 and 350 characters. Does this correspond to another word limit, or is there something else happening? We can print out the highest frequency texts for each funder: It\u2019s pretty clear that there is some placeholder text that is repeated many times throughout the dataset. Cleaning From all of this information, we can make some decisions about data cleaning: Drop all projects with abstracts that have fewer than 150 characters Drop projects where the abstract matches any of the placeholder texts For good measure, we will also drop all but one of any duplicate abstracts Step 2 is an example of where we will use the aux data folder . This is reserved for small pieces of information that have been manually created and are used in any preprocessing, analysis or modelling. In this case, we will save the two abstracts in a .txt file. Now we can prototype our cleaning function in the notebook and test it on the dataframe. Looks good. Pre-processing The last step is to prepare our abstracts for modelling by cleaning and tokenizing the raw text. To do that, copy and paste this chunk of code below into a cell and run it. Finally, we can check that this works by applying it to the text. Looks good. From here we\u2019re ready to take our first step to factoring out our code into a more reproducible and shareable form. After exploration and writing cleaning functions, we need to preprocess the text. \u23f8 Pause - Lab Book Remember near the beginning, we said we would keep a lab book of what we were doing? So far we\u2019ve just been forging ahead with coding. That\u2019s great, but before we go too far, we should record what we\u2019ve done. A lab book is a personal thing and it's up to you how you keep it whether it be paper, google docs, markdown etc. What is most important is that you keep one. Write down what you tried (even if it failed), references to useful blogs/papers, fixes to bugs/install issues, and issues in the data you discover as you go. We suggest that you review your lab-book regularly, e.g. daily or weekly, and add information relevant to others as issues/comments on an issue. One particularly important thing to document in a common issue thread are any outliers/issues/odd entries in a dataset. Refactoring Now at the end of the make_dataset stage (before we apply/train any models to generate features). Here we factor out our functions from our notebooks, write any docstrings necessary, update data Readme's etc. data/README.md Write the data Readme . baking_cookies/data/make_dataset.py Write a function which collects or reads the raw data .env file points to config location of the MySQL config file Also use model_config.yaml which is tracked to get parameters for data processing baking_cookies/data/sql.py Put our generic get_engine function here and add a #UTILS flag to the docstring as this is a reusable component that we don't wish to keep repeating across repositories. baking_cookies/data/make_gtr.py Factor out the data processing functions from the notebook Defines file in and file out Loads in any aux data baking_cookies/features/text_preprocessing.py Generic functions for text preprocessing that might be applied to other data too Add #UTILS flag This is a good point to make a PR. Add someone to review. In the mean time you can get on with other tasks etc. (put \u201ccloses # \u201d in commit message) Guidelines We have a few guidelines to suggest how you should structure files. If you find these are not sensible for your use-case please report this on this issue . Segregate functions into files/folders based on functionality and then dataset where possible. You might want to put functionality relating to: data in baking_cookies/data ; functionality that produces features in baking_cookies/features ; functionality that produces models in baking_cookies/models . Each high-level component (e.g. the creation of the cleaned and pre-processed Gateway to Research dataset) should be callable in one function with a prefix make_ that accepts paths and parameters as arguments. make_ functions should perform any necessary setup IO work (e.g. read data from disk), call a function with a process_ prefix, then perform any necessary IO tear-down work (e.g. save data to disk). process_ functions should accept data and parameters (i.e. not paths) and return data - they should not perform IO! It may not always be possible to adhere to this pattern but stick to it where possible. ( make_ and process_ are terrible names which need to be renamed - perhaps make_ and run_ ? discuss here ) Parameters should not be hard-coded but specified in model_config.yaml , loaded from the dictionary baking_cookies.config , and passed into the make_ prefixed function. You could load the config in the make_ function but consider whether this decreases transparency - less informative doc-strings for make_ - or generalisability. Don't use print , use the logging module, e.g. logger = logging.getLogger(__name__); logger.info('Logging at info level goes into `info.log`') Training a model We now have a cleaned dataset so now we wish to proceed with prototyping a model that predicts the lead Funder from the abstract texts. The steps we need to follow are: Make an issue and new branch for model training Create a new notebook Load cleaned data 4. Train test split and save outputs (IDs) 5. Create model pipeline (for training purposes) 6. Fit and evaluate (at this point you might copy paste evaluation to lab book) 7. Save model 8. Add notebooks into WIP branch 9. Factor out code and make a PR into gtr-projects The model is very simple, it calculates TF-IDF features for each abstract and performs a logistic regression. Take a look through the notebook to see the prototyping of the model, and compare this to the factored out code to understand where each of the constituent parts have gone. We have split up the model pipeline into its constituent parts. This means that changes can be made to one part of the model pipeline (e.g. the train-test split strategy/size) without having to worry about searching through the full pipeline code. This also makes code easier to test. The constituent parts we have chosen are: Train-test split Training the model Evaluating the model Again we follow the make_ , process_ conventions; however in train_model.py the model parameters are loaded in make_train_model() . Exercise: Change this so that the config parameters are loaded in __main__ , and add an option to pass in whether a grid_search is True or False from the command line using the click library (Hint: see ~make dataset.py ). Publishable notebooks [Incomplete] Share interim analyses as Gists Save final notebooks as html files and put in reports/ . These are for presenting results and thus should: Document the purpose of the notebook at the top Provide discussion throughout the document Import and run functions - not define them Ideally be readable and understandable with the code hidden (unless you are demonstrating code usage of course) Testing [Incomplete] Tools/tips [Incomplete] nbextensions + configurator Gist-it Autopep8 Hide input Select CodeMirror Keymap ExecuteTime Magics Full list here %debug %pdb %env / %set_env %run %psearch %autoreload Profilers %prun mprof - https://pypi.org/project/memory-profiler/","title":"Tutorial"},{"location":"tutorial/#introduction","text":"In this tutorial we are going to cover the workflow for creating a supervised model. In this instance, we will train a classifier to predict the name of the body who funded a research project from Gateway to Research based on its abstract text. The main aims of this tutorial are to learn how to structure the repository, prototype, and factor out code into modules so that the model and any processed data can be reproduced by anyone else with a simple command. In this tutorial, we will: Collect raw data from MySQL Explore the data in a notebook Write a data README Clean and preprocess the data Factor out cleaning and preprocessing code into modules Prototype feature extraction and model in a notebook Train and save the modelling pipeline Factor out feature extraction and modelling code into modules Put model and data under version control Update model Along the way, we should also keep a lab book, to document what we\u2019ve been doing.","title":"Introduction"},{"location":"tutorial/#the-repository","text":"To begin with, clone the repository into a directory on your machine with the git clone command: $ git clone https://github.com/nestauk/baking-cookies.git $ # OR: git clone git@github.com:nestauk/baking-cookies.git Move in to the repository directory: $ cd baking-cookies If you type git branch , you can see that you are on the master branch. In general, we want to maintain the master branch for completely finished work, so we will work from the dev branch instead (within the settings for a Github repository you can change the default branch from master if you wish). $ git checkout dev From dev we will create two more branches. One will be for the sub-project - creating our model - and another will be your personal branch for work in progress (WIP). We make this second one because it is possible that both you and someone else will be prototyping for this sub-project at the same time, and it will be better if you have separate spaces to do this. For example, this allows you to backup your work by pushing to the remote without worrying about someone else pulling in those changes. Do not use someone else's personal branch : personal branches are liable to be deleted/rebased/make breaking changes. Use the -b flag to create and checkout your new branches simultaneously. I\u2019m going to use my initials (\"gr\") for the second branch, but you can use your own if you prefer. $ git checkout -b gtr-predict-funder $ git checkout -b gtr-predict-funder-gr-wip On your personal branch, you can prototype in notebooks that aren\u2019t intended for mass readership (though you may still want to make them readable enough to show collaborators). This is where we will work until we\u2019re ready to write more polished code and produce models. While we\u2019re getting set up, we should install the repository package in to the Python environment you want to use. $ pip install -e . We recommend (disk space permitting) that you have one environment per project. You can initialise a new conda environment by running make create_environment , and install the environment requirements in conda_environment.yaml with make requirements . When adding requirements to conda_environment.yaml only add the major dependencies (e.g. pandas not everything it depends on), and do not restrict the version numbers too much (e.g. - pandas=0.24.2 rather than - pandas=0.24.2=py37he6710b0_0 ) - this will avoid difficulties with other people on different operating systems not being able to reproduce your environment.","title":"The Repository"},{"location":"tutorial/#loading-cleaning-and-preprocessing","text":"You might have noticed that there is a notebooks folder in the project root directory. In here, there is a subdirectory, dev . This folder is for any notebooks that can be considered work in progress. See our guide to using notebooks . Launch jupyter and create a notebook in notebooks/dev/ for cleaning and processing the data. With our naming convention in mind, we will call this notebook 01_gr_gtr_projects .","title":"Loading, Cleaning and Preprocessing"},{"location":"tutorial/#loading","text":"The data we need for this tutorial is on Nesta\u2019s MySQL database. From the screenshot below you can see how we will write some simple code to grab the table we need, but there are also a few important things to note: The first cell in the notebook magic runs the preamble script. This loads some useful data science libraries and runs some useful boilerplate code so that we can quickly access variables such as the project directory. You can adapt this script to your project\u2019s needs. We hard code config_path to point to the location of our database config file. This is fine for now because we are working in a notebook that no one else is expected to be able to run seamlessly, but we will change it later. After collecting the data, we show the head of the dataframe. Printing and showing outputs at stages along the prototyping should be standard practice so that if you do show someone else the notebook, they can easily see what\u2019s going on, without needing to run the code. After loading and inspecting the data we will add another cell that saves it locally as a flat file to our raw directory. projects.to_csv(f'{data_path}/raw/gtr_projects.csv', index=False)","title":"Loading"},{"location":"tutorial/#eda","text":"All new data should be explored before it is worked with in any serious capacity. NOTE : The EDA in this tutorial is not very rigorous. More rigorous guidelines for EDA are on the roadmap . This section of the tutorial will be updated accordingly in the future. Here we will use pandas_profiling and some more specific exploration. If you don\u2019t have it you can easily install it with $ pip install pandas-profiling The profiler produces a standard output, and covers many data quality elements. It\u2019s likely to be useful for anyone else looking at this dataset, so we will also save it in the reports directory as a html file. Now we can dive in to the specific features that we\u2019re interested in. First we look at the distribution of funders. It looks like we have fairly good representation from all of the funders, with over 5,000 projects for each of them apart from NC3Rs. Let\u2019s also look at the distribution of abstract text lengths. There\u2019s a peak at around 4,000 characters which presumably relates to a length limitation imposed on submitted abstracts. However, there\u2019s also a peak close to 0. Let\u2019s take a look at the distributions by funder and also have a closer look at that low end. It looks like there\u2019s some projects with very few characters which we will probably discard. However, there are a high frequency of abstracts with lengths at around 250 and 350 characters. Does this correspond to another word limit, or is there something else happening? We can print out the highest frequency texts for each funder: It\u2019s pretty clear that there is some placeholder text that is repeated many times throughout the dataset.","title":"EDA"},{"location":"tutorial/#cleaning","text":"From all of this information, we can make some decisions about data cleaning: Drop all projects with abstracts that have fewer than 150 characters Drop projects where the abstract matches any of the placeholder texts For good measure, we will also drop all but one of any duplicate abstracts Step 2 is an example of where we will use the aux data folder . This is reserved for small pieces of information that have been manually created and are used in any preprocessing, analysis or modelling. In this case, we will save the two abstracts in a .txt file. Now we can prototype our cleaning function in the notebook and test it on the dataframe. Looks good.","title":"Cleaning"},{"location":"tutorial/#pre-processing","text":"The last step is to prepare our abstracts for modelling by cleaning and tokenizing the raw text. To do that, copy and paste this chunk of code below into a cell and run it. Finally, we can check that this works by applying it to the text. Looks good. From here we\u2019re ready to take our first step to factoring out our code into a more reproducible and shareable form. After exploration and writing cleaning functions, we need to preprocess the text.","title":"Pre-processing"},{"location":"tutorial/#pause-lab-book","text":"Remember near the beginning, we said we would keep a lab book of what we were doing? So far we\u2019ve just been forging ahead with coding. That\u2019s great, but before we go too far, we should record what we\u2019ve done. A lab book is a personal thing and it's up to you how you keep it whether it be paper, google docs, markdown etc. What is most important is that you keep one. Write down what you tried (even if it failed), references to useful blogs/papers, fixes to bugs/install issues, and issues in the data you discover as you go. We suggest that you review your lab-book regularly, e.g. daily or weekly, and add information relevant to others as issues/comments on an issue. One particularly important thing to document in a common issue thread are any outliers/issues/odd entries in a dataset.","title":"\u23f8 Pause - Lab Book"},{"location":"tutorial/#refactoring","text":"Now at the end of the make_dataset stage (before we apply/train any models to generate features). Here we factor out our functions from our notebooks, write any docstrings necessary, update data Readme's etc.","title":"Refactoring"},{"location":"tutorial/#datareadmemd","text":"Write the data Readme .","title":"data/README.md"},{"location":"tutorial/#baking_cookiesdatamake_datasetpy","text":"Write a function which collects or reads the raw data .env file points to config location of the MySQL config file Also use model_config.yaml which is tracked to get parameters for data processing","title":"baking_cookies/data/make_dataset.py"},{"location":"tutorial/#baking_cookiesdatasqlpy","text":"Put our generic get_engine function here and add a #UTILS flag to the docstring as this is a reusable component that we don't wish to keep repeating across repositories.","title":"baking_cookies/data/sql.py"},{"location":"tutorial/#baking_cookiesdatamake_gtrpy","text":"Factor out the data processing functions from the notebook Defines file in and file out Loads in any aux data","title":"baking_cookies/data/make_gtr.py"},{"location":"tutorial/#baking_cookiesfeaturestext_preprocessingpy","text":"Generic functions for text preprocessing that might be applied to other data too Add #UTILS flag This is a good point to make a PR. Add someone to review. In the mean time you can get on with other tasks etc. (put \u201ccloses # \u201d in commit message)","title":"baking_cookies/features/text_preprocessing.py"},{"location":"tutorial/#guidelines","text":"We have a few guidelines to suggest how you should structure files. If you find these are not sensible for your use-case please report this on this issue . Segregate functions into files/folders based on functionality and then dataset where possible. You might want to put functionality relating to: data in baking_cookies/data ; functionality that produces features in baking_cookies/features ; functionality that produces models in baking_cookies/models . Each high-level component (e.g. the creation of the cleaned and pre-processed Gateway to Research dataset) should be callable in one function with a prefix make_ that accepts paths and parameters as arguments. make_ functions should perform any necessary setup IO work (e.g. read data from disk), call a function with a process_ prefix, then perform any necessary IO tear-down work (e.g. save data to disk). process_ functions should accept data and parameters (i.e. not paths) and return data - they should not perform IO! It may not always be possible to adhere to this pattern but stick to it where possible. ( make_ and process_ are terrible names which need to be renamed - perhaps make_ and run_ ? discuss here ) Parameters should not be hard-coded but specified in model_config.yaml , loaded from the dictionary baking_cookies.config , and passed into the make_ prefixed function. You could load the config in the make_ function but consider whether this decreases transparency - less informative doc-strings for make_ - or generalisability. Don't use print , use the logging module, e.g. logger = logging.getLogger(__name__); logger.info('Logging at info level goes into `info.log`')","title":"Guidelines"},{"location":"tutorial/#training-a-model","text":"We now have a cleaned dataset so now we wish to proceed with prototyping a model that predicts the lead Funder from the abstract texts. The steps we need to follow are: Make an issue and new branch for model training Create a new notebook Load cleaned data 4. Train test split and save outputs (IDs) 5. Create model pipeline (for training purposes) 6. Fit and evaluate (at this point you might copy paste evaluation to lab book) 7. Save model 8. Add notebooks into WIP branch 9. Factor out code and make a PR into gtr-projects The model is very simple, it calculates TF-IDF features for each abstract and performs a logistic regression. Take a look through the notebook to see the prototyping of the model, and compare this to the factored out code to understand where each of the constituent parts have gone. We have split up the model pipeline into its constituent parts. This means that changes can be made to one part of the model pipeline (e.g. the train-test split strategy/size) without having to worry about searching through the full pipeline code. This also makes code easier to test. The constituent parts we have chosen are: Train-test split Training the model Evaluating the model Again we follow the make_ , process_ conventions; however in train_model.py the model parameters are loaded in make_train_model() .","title":"Training a model"},{"location":"tutorial/#exercise","text":"Change this so that the config parameters are loaded in __main__ , and add an option to pass in whether a grid_search is True or False from the command line using the click library (Hint: see ~make dataset.py ).","title":"Exercise:"},{"location":"tutorial/#publishable-notebooks-incomplete","text":"Share interim analyses as Gists Save final notebooks as html files and put in reports/ . These are for presenting results and thus should: Document the purpose of the notebook at the top Provide discussion throughout the document Import and run functions - not define them Ideally be readable and understandable with the code hidden (unless you are demonstrating code usage of course)","title":"Publishable notebooks [Incomplete]"},{"location":"tutorial/#testing-incomplete","text":"","title":"Testing [Incomplete]"},{"location":"tutorial/#toolstips-incomplete","text":"","title":"Tools/tips [Incomplete]"},{"location":"tutorial/#nbextensions-configurator","text":"","title":"nbextensions + configurator"},{"location":"tutorial/#gist-it","text":"","title":"Gist-it"},{"location":"tutorial/#autopep8","text":"","title":"Autopep8"},{"location":"tutorial/#hide-input","text":"","title":"Hide input"},{"location":"tutorial/#select-codemirror-keymap","text":"","title":"Select CodeMirror Keymap"},{"location":"tutorial/#executetime","text":"","title":"ExecuteTime"},{"location":"tutorial/#magics","text":"Full list here","title":"Magics"},{"location":"tutorial/#debug","text":"","title":"%debug"},{"location":"tutorial/#pdb","text":"","title":"%pdb"},{"location":"tutorial/#env-set_env","text":"","title":"%env / %set_env"},{"location":"tutorial/#run","text":"","title":"%run"},{"location":"tutorial/#psearch","text":"","title":"%psearch"},{"location":"tutorial/#autoreload","text":"","title":"%autoreload"},{"location":"tutorial/#profilers","text":"%prun mprof - https://pypi.org/project/memory-profiler/","title":"Profilers"}]}