{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nesta Data Science Cookiecutter A standard project structure for reproducible and collaborative data science projects @ Nesta. High-level aims Enable data-scientists @ Nesta to work with each other Increase reliability of data-science @ Nesta Make our projects more reproducible Allow data-scientists to hand off to data-engineers Give data-scientists easy access to cloud computing (Light) Data version control Increase the value of codebases, and accompanying documentation/reports to stakeholders Keep reporting of results in sync with codebase Facilitate code-reuse and refactoring into *_DAPS and ds-utils by reducing coupling and increasing code quality Make code easier to understand Whilst retaining as much agility and freedom to explore as possible. Contents Just get me started with a new project (QUICKSTART) Tell me where X goes Show me some example project structures FAQ Roadmap","title":"Home"},{"location":"#nesta-data-science-cookiecutter","text":"A standard project structure for reproducible and collaborative data science projects @ Nesta.","title":"Nesta Data Science Cookiecutter"},{"location":"#high-level-aims","text":"Enable data-scientists @ Nesta to work with each other Increase reliability of data-science @ Nesta Make our projects more reproducible Allow data-scientists to hand off to data-engineers Give data-scientists easy access to cloud computing (Light) Data version control Increase the value of codebases, and accompanying documentation/reports to stakeholders Keep reporting of results in sync with codebase Facilitate code-reuse and refactoring into *_DAPS and ds-utils by reducing coupling and increasing code quality Make code easier to understand Whilst retaining as much agility and freedom to explore as possible.","title":"High-level aims"},{"location":"#contents","text":"Just get me started with a new project (QUICKSTART) Tell me where X goes Show me some example project structures FAQ Roadmap","title":"Contents"},{"location":"examples/","text":"Example project structures Coming soon.","title":"Example project structures"},{"location":"examples/#example-project-structures","text":"Coming soon.","title":"Example project structures"},{"location":"faq/","text":"FAQ Please submit questions as a Github issue with the label \"question\". Where should I save models? If it's a pre-trained model someone has sent you: inputs/models/ If it's a model you have trained: outputs/models/","title":"FAQ"},{"location":"faq/#faq","text":"Please submit questions as a Github issue with the label \"question\".","title":"FAQ"},{"location":"faq/#where-should-i-save-models","text":"If it's a pre-trained model someone has sent you: inputs/models/ If it's a model you have trained: outputs/models/","title":"Where should I save models?"},{"location":"guidelines/","text":"Technical and working style guidelines Challenging this documentation is encouraged. Please set up an issue for challenges, additions or corrections. For explanations, please consult the dev Slack channel In this document we set out some basic \"tips\" (either mandatory or encouraged) which should guide your way of working, regardless of the task. Foreword python is not pandas Design patterns Programming Critical thinking Naming conventions Spaces and spacing Comments and docs Foreword In advance, we recommend installing the autoformatter black in your IDE. In future we will use flake8 (or similar) for automatically checking our python codebases. Hopefully the conventions laid out here are the easy and intuitive set of pep8 . Code reviewers: it is on you to ensure that this style guide has been followed: there are no points for being lenient, but there [non-redeemable] points for being opinionated! We should all feel pressured into making sure that our code meets an acceptable standard. python is not pandas tldr; Using pandas as a means to perform transformations or calculations on your data should be avoided, unless it clearly simplifies the logic and readability of your code. That is not so say that you should not use pandas , but rather that you justify to yourself that pandas isn't making your life harder in lieu of using standard python tools. We appreciate that pandas is a gateway into python programming for many people, and for that reason it becomes habitual way of coding. However... code containing lots of pandas operations are almost impossible to review, and therefore have the capacity to accumulate vast numbers of bugs. In general, pandas makes column-wise operations and IO (reading/writing files) dead easy. That said, pandas column-wise operations are inherited from numpy , and numpy is generally accepted in the place of dataframes. pandas is enormous, in many ways. If it can be omitted from your code then you can make big savings in terms of memory usage and requirements clashes, and even CPU time. Instead of Googling how to achieve something in pandas with an obscure chaining of functions, break the problem down and solve it yourself. It is highly unlikely that the pandas approach to reshaping your data will beat using tools from numpy , itertools , functools and toolz , even if you switch to representing data in numpy arrays or even as list of dict ( [{'value': 21}, {'value': 45}] ). If you would like guidance, tips or ideas on how to un panda your code then ask on the dev Slack Channel - we're all here to help! Design patterns Favour the following design patterns, in this order: Functional programming: using functions to do one thing well, not altering the original data. Modular code: using functions to eliminate duplication, improve readability and reduce indentation. Object-oriented programming: Generally avoid, unless you are customising an API (for example DataFrame ) or defining your own API. If you are not, at least, adhering to a modular style then you have gone very wrong. You should implement unit tests for each of your functions, something which is generally more tricky for object-oriented programming. Programming Mandatory NB: eventually these checks will be automatic Don't compare boolean values to True or False . Favour is not condition over not condition is Don't compare a value to None ( value == None ), always favour value is None Encouraged Favour logging over print Favour using configuration files, or (faster/lazier/less good/ok) GLOBAL_VARIABLES near the top of your code, rather than repeated use of hard-coded variables in your code, particularly when with URL and file path variables like s3://path/to/some/fixed/place , but also for repeated numeric hyperparameters. Critical thinking The following questions should be going through your mind when developing your code. If you don't quite understand the wording or intention of the following questions then we encourage you to ask in the dev Slack channel! Surely this simple problem already has an elegant solution? How many copies of the data am I making in memory? Where do my variables go out of scope? How can I avoid creating a new variable at all costs? (think iterator, scope, lru_cache ) Have I made sure that I only run expensive or time consuming processes as few times as possible? Naming conventions Mandatory Functions / methods: function , my_function (snake case) Variables / attributes: variable , my_var (snake case) Class: Model , MyClass (camel case) Module / file names / directory names: module , file_name.py , dir_name (camel case) Global* / constants: A_GLOBAL_VARIABLE (screaming snake case) * here we use \"Global\" to mean constants in scope at the module level, not the global level. Don't use global , ever. Encouraged Keep all names as short and descriptive as possible. Variable names such as x or df are highly discouraged unless they are genuinely representing abstract concepts. Favour good naming conventions over helpful comments Spaces and spacing NB: that using the autoformatter black in your IDE will resolve almost all of the following Encouraged Use the absolute minimum number of indents of your code. You can achieve this by writing modular code, and inverting logic, for example: def something(args): for item_collection in args: # 1 tab if item_collection.exists(): # 2 tabs the_sum = 0 # 3 tabs for item in item_collection: the_sum += item.value() # 4 tabs print(the_sum) can become: def sum_values(item_collection): the_sum = sum(item.value() for item in item_collection) print(the_sum) def something(args): for item_collection in args: # 1 tab if not item_collection.exists(): # 2 tabs continue # 3 tabs, buts unindents following lines sum_values(item_collection) # 2 tabs Put a space before starting block comments # like this , #not this Inline comments need two spaces before them a = 2 # like this Keep lines to 88 (officially 79) characters or less. You can achieve this by utilising other parts of this guideline, particularly with regards to creating modular code. Splitting over multiple lines is, of course, permissible so long as it doesn't conflict with legibility. When declaring default values, never put spaces around operators like = , i.e def this_is_ok(param=1) , def this_is_NOT_ok(param = 1) . Otherwise, all operators must always have a single space on either side. Separate function and class arguments with a comma and a space, i.e. do_thing(1, b=2) and not do_thing(1,b=2) . Comments and docs Mandatory At least a basic docstring is required for every function/method and class Full, explanatory docstrings are required for all function/methods and classes if it will be used in the main body of a code routine. Use Google-style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html If you are using type hints then then you can write e.g. my_arg (str): description as simply my_arg: description . Encouraged Don't state the obvious in comments Before writing a comment, consider whether that information would be better be encoded in a useful variable or function name. Workflow This builds on a much greater body of work, laid out in nestauk/github_support . For avoidance of doubt, branches must be linked to a GitHub issue and named accordingly: {GitHub issue number}_{tinyLowerCamelDescription} For example 14_readme , which indicates that this branch refered to this issue . You should generally favour having a dev branch, in addition to your main ( master ) branch. Never commit to dev , master or main . Only pull requests from branches named {GitHub issue number}_{tinyLowerCamelDescription} should ever be accepted. Please make all PRs and issues reasonably small: they should be trying to achieve roughly one task. Inevitably some simple tasks spawn large numbers of utils, and sometimes these detract from the original PR. In this case, you should stack an new PR on top of your \"base\" PR, for example as {GitHub issue number}_{differentLowerCamelDescription} . In this case the PR / Git merging tree will look like: dev <-- 123_originalThing <-- 423_differentThing <-- 578_anotherDifferentThing We can then merge the PR 123_originalThing into dev , then 423_differentThing into dev (after calling git merge dev on 423_differentThing ), etc until the chain is merged entirely. The nominated reviewer should review the entire chain, before the merge can go ahead. PRs should only be merged if all tests and a review has been signed off.","title":"Python guidelines"},{"location":"guidelines/#technical-and-working-style-guidelines","text":"Challenging this documentation is encouraged. Please set up an issue for challenges, additions or corrections. For explanations, please consult the dev Slack channel In this document we set out some basic \"tips\" (either mandatory or encouraged) which should guide your way of working, regardless of the task. Foreword python is not pandas Design patterns Programming Critical thinking Naming conventions Spaces and spacing Comments and docs","title":"Technical and working style guidelines"},{"location":"guidelines/#foreword","text":"In advance, we recommend installing the autoformatter black in your IDE. In future we will use flake8 (or similar) for automatically checking our python codebases. Hopefully the conventions laid out here are the easy and intuitive set of pep8 . Code reviewers: it is on you to ensure that this style guide has been followed: there are no points for being lenient, but there [non-redeemable] points for being opinionated! We should all feel pressured into making sure that our code meets an acceptable standard.","title":"Foreword"},{"location":"guidelines/#python-is-not-pandas","text":"tldr; Using pandas as a means to perform transformations or calculations on your data should be avoided, unless it clearly simplifies the logic and readability of your code. That is not so say that you should not use pandas , but rather that you justify to yourself that pandas isn't making your life harder in lieu of using standard python tools. We appreciate that pandas is a gateway into python programming for many people, and for that reason it becomes habitual way of coding. However... code containing lots of pandas operations are almost impossible to review, and therefore have the capacity to accumulate vast numbers of bugs. In general, pandas makes column-wise operations and IO (reading/writing files) dead easy. That said, pandas column-wise operations are inherited from numpy , and numpy is generally accepted in the place of dataframes. pandas is enormous, in many ways. If it can be omitted from your code then you can make big savings in terms of memory usage and requirements clashes, and even CPU time. Instead of Googling how to achieve something in pandas with an obscure chaining of functions, break the problem down and solve it yourself. It is highly unlikely that the pandas approach to reshaping your data will beat using tools from numpy , itertools , functools and toolz , even if you switch to representing data in numpy arrays or even as list of dict ( [{'value': 21}, {'value': 45}] ). If you would like guidance, tips or ideas on how to un panda your code then ask on the dev Slack Channel - we're all here to help!","title":"python is not pandas"},{"location":"guidelines/#design-patterns","text":"Favour the following design patterns, in this order: Functional programming: using functions to do one thing well, not altering the original data. Modular code: using functions to eliminate duplication, improve readability and reduce indentation. Object-oriented programming: Generally avoid, unless you are customising an API (for example DataFrame ) or defining your own API. If you are not, at least, adhering to a modular style then you have gone very wrong. You should implement unit tests for each of your functions, something which is generally more tricky for object-oriented programming.","title":"Design patterns"},{"location":"guidelines/#programming","text":"","title":"Programming"},{"location":"guidelines/#mandatory","text":"NB: eventually these checks will be automatic Don't compare boolean values to True or False . Favour is not condition over not condition is Don't compare a value to None ( value == None ), always favour value is None","title":"Mandatory"},{"location":"guidelines/#encouraged","text":"Favour logging over print Favour using configuration files, or (faster/lazier/less good/ok) GLOBAL_VARIABLES near the top of your code, rather than repeated use of hard-coded variables in your code, particularly when with URL and file path variables like s3://path/to/some/fixed/place , but also for repeated numeric hyperparameters.","title":"Encouraged"},{"location":"guidelines/#critical-thinking","text":"The following questions should be going through your mind when developing your code. If you don't quite understand the wording or intention of the following questions then we encourage you to ask in the dev Slack channel! Surely this simple problem already has an elegant solution? How many copies of the data am I making in memory? Where do my variables go out of scope? How can I avoid creating a new variable at all costs? (think iterator, scope, lru_cache ) Have I made sure that I only run expensive or time consuming processes as few times as possible?","title":"Critical thinking"},{"location":"guidelines/#naming-conventions","text":"","title":"Naming conventions"},{"location":"guidelines/#mandatory_1","text":"Functions / methods: function , my_function (snake case) Variables / attributes: variable , my_var (snake case) Class: Model , MyClass (camel case) Module / file names / directory names: module , file_name.py , dir_name (camel case) Global* / constants: A_GLOBAL_VARIABLE (screaming snake case) * here we use \"Global\" to mean constants in scope at the module level, not the global level. Don't use global , ever.","title":"Mandatory"},{"location":"guidelines/#encouraged_1","text":"Keep all names as short and descriptive as possible. Variable names such as x or df are highly discouraged unless they are genuinely representing abstract concepts. Favour good naming conventions over helpful comments","title":"Encouraged"},{"location":"guidelines/#spaces-and-spacing","text":"NB: that using the autoformatter black in your IDE will resolve almost all of the following","title":"Spaces and spacing"},{"location":"guidelines/#encouraged_2","text":"Use the absolute minimum number of indents of your code. You can achieve this by writing modular code, and inverting logic, for example: def something(args): for item_collection in args: # 1 tab if item_collection.exists(): # 2 tabs the_sum = 0 # 3 tabs for item in item_collection: the_sum += item.value() # 4 tabs print(the_sum) can become: def sum_values(item_collection): the_sum = sum(item.value() for item in item_collection) print(the_sum) def something(args): for item_collection in args: # 1 tab if not item_collection.exists(): # 2 tabs continue # 3 tabs, buts unindents following lines sum_values(item_collection) # 2 tabs Put a space before starting block comments # like this , #not this Inline comments need two spaces before them a = 2 # like this Keep lines to 88 (officially 79) characters or less. You can achieve this by utilising other parts of this guideline, particularly with regards to creating modular code. Splitting over multiple lines is, of course, permissible so long as it doesn't conflict with legibility. When declaring default values, never put spaces around operators like = , i.e def this_is_ok(param=1) , def this_is_NOT_ok(param = 1) . Otherwise, all operators must always have a single space on either side. Separate function and class arguments with a comma and a space, i.e. do_thing(1, b=2) and not do_thing(1,b=2) .","title":"Encouraged"},{"location":"guidelines/#comments-and-docs","text":"","title":"Comments and docs"},{"location":"guidelines/#mandatory_2","text":"At least a basic docstring is required for every function/method and class Full, explanatory docstrings are required for all function/methods and classes if it will be used in the main body of a code routine. Use Google-style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html If you are using type hints then then you can write e.g. my_arg (str): description as simply my_arg: description .","title":"Mandatory"},{"location":"guidelines/#encouraged_3","text":"Don't state the obvious in comments Before writing a comment, consider whether that information would be better be encoded in a useful variable or function name.","title":"Encouraged"},{"location":"guidelines/#workflow","text":"This builds on a much greater body of work, laid out in nestauk/github_support . For avoidance of doubt, branches must be linked to a GitHub issue and named accordingly: {GitHub issue number}_{tinyLowerCamelDescription} For example 14_readme , which indicates that this branch refered to this issue . You should generally favour having a dev branch, in addition to your main ( master ) branch. Never commit to dev , master or main . Only pull requests from branches named {GitHub issue number}_{tinyLowerCamelDescription} should ever be accepted. Please make all PRs and issues reasonably small: they should be trying to achieve roughly one task. Inevitably some simple tasks spawn large numbers of utils, and sometimes these detract from the original PR. In this case, you should stack an new PR on top of your \"base\" PR, for example as {GitHub issue number}_{differentLowerCamelDescription} . In this case the PR / Git merging tree will look like: dev <-- 123_originalThing <-- 423_differentThing <-- 578_anotherDifferentThing We can then merge the PR 123_originalThing into dev , then 423_differentThing into dev (after calling git merge dev on 423_differentThing ), etc until the chain is merged entirely. The nominated reviewer should review the entire chain, before the merge can go ahead. PRs should only be merged if all tests and a review has been signed off.","title":"Workflow"},{"location":"quickstart/","text":"Quickstart Requirements Python 3.6+ A *NIX system (e.g. Linux/macOS) - Windows might work, but we don't support it Mac users: The commands below assume you have homebrew installed Cookiecutter Python package >= 1.4.0: pip install cookiecutter git-crypt - required for metaflow on AWS brew install git-crypt # mac apt-get install -y git-crypt # Ubuntu linux: github CLI [ OPTIONAL - required if auto_config set to true (the default)] - for automatic creation and configuration of a Github repo Install Mac: brew install gh Linux: https://github.com/cli/cli/blob/trunk/docs/install_linux.md Configure : gh auth login and answer prompts as below: What account do you want to log into? Github.com What is your preferred protocol for Git operations? SSH Upload your SSH public key to your Github account? Select the key you used to sign-up for 2-factor authentication with github How would you like to authenticate GitHub CLI? Login with a web browser Have a Nesta AWS account configured with awscli Note to any non-Nesta visitors: You can still use the cookiecutter by: Choosing auto_config as \"false\" when setting up the cookiecutter Remove the setup-metaflow dependency of the install command within Makefile If using make init - replace references to nestauk with your github organisation in bin/create_repo.sh We recommend taking the time to install and configure the optional dependencies as this one-time setup allows you to use the auto_config option which should save you a lot of time and avoids human error during configuration. Starting from scratch Create Ensure you have installed the requirements and then run cookiecutter https://github.com/nestauk/ds-cookiecutter . This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name : Type the name of your project here repo_name : Type the name of your repository here The default is a processed version of project_name that is compatible with python package names author_name : The author of the project description : A short description of your project openness : Whether the project is \"public\" (default) or \"private\" You should only make a project private if you have a good reason If \"public\" then an MIT license will be generated; otherwise the license will be a copyright placeholder If you choose auto_config as \"true\" then the Github repo created will obey this setting s3_bucket : The name of an S3 bucket to store assets for this project If you choose auto_config as \"true\" then this bucket will be created for you Careful : This needs to not conflict with any existing s3 bucket name This value can be reconfigured in .env.shared github_account : The github account that this project will be created in auto_config : Whether to automatically create a conda environment; github repo; S3 bucket; and configure AWS with metaflow. Requires optional requirement gh (the Github cli) to have been installed and configured (with gh auth login ) Configure If you selected auto_config as \"true\", the following actions have happened: A conda environment, project_name , has been created (with the project package installed in editable mode) Git pre-commit hooks have been configured and installed The Nesta metaflow config has been fetched and decrypted. It should exist in ~/.metaflowconfig/config.json A github repo github.com/nestauk/project_name has been created and configured An s3 bucket project_name has been created If you selected auto_config as \"false\", you will need to do the above manually (or run make init ): Run make install to configure the development environment: Setup the conda environment Configure pre-commit Configure metaflow to use AWS Manually create an S3 bucket s3_bucket (or run bash bin/create_bucket.sh ) S3 names must only contain lowercase letters, numbers, dots, and hyphens. Manually create a github repository (or run bash bin/create_repo.sh ) Collaborating on an existing project Clone the repository and cd into the repository. Run make install to configure the development environment: Setup the conda environment Install dependencies and local package Configure pre-commit Configure metaflow to use AWS conda activate project_name Check the project's README for any additional configuration needed (e.g. putting API keys in .env ) Pull any required inputs into inputs/ by running make inputs-pull Follow the author's documentation, or make them write some if they haven't already!","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#requirements","text":"Python 3.6+ A *NIX system (e.g. Linux/macOS) - Windows might work, but we don't support it Mac users: The commands below assume you have homebrew installed Cookiecutter Python package >= 1.4.0: pip install cookiecutter git-crypt - required for metaflow on AWS brew install git-crypt # mac apt-get install -y git-crypt # Ubuntu linux: github CLI [ OPTIONAL - required if auto_config set to true (the default)] - for automatic creation and configuration of a Github repo Install Mac: brew install gh Linux: https://github.com/cli/cli/blob/trunk/docs/install_linux.md Configure : gh auth login and answer prompts as below: What account do you want to log into? Github.com What is your preferred protocol for Git operations? SSH Upload your SSH public key to your Github account? Select the key you used to sign-up for 2-factor authentication with github How would you like to authenticate GitHub CLI? Login with a web browser Have a Nesta AWS account configured with awscli Note to any non-Nesta visitors: You can still use the cookiecutter by: Choosing auto_config as \"false\" when setting up the cookiecutter Remove the setup-metaflow dependency of the install command within Makefile If using make init - replace references to nestauk with your github organisation in bin/create_repo.sh We recommend taking the time to install and configure the optional dependencies as this one-time setup allows you to use the auto_config option which should save you a lot of time and avoids human error during configuration.","title":"Requirements"},{"location":"quickstart/#starting-from-scratch","text":"","title":"Starting from scratch"},{"location":"quickstart/#create","text":"Ensure you have installed the requirements and then run cookiecutter https://github.com/nestauk/ds-cookiecutter . This opens a series of prompts to configure your new project (values in square brackets denote defaults): project_name : Type the name of your project here repo_name : Type the name of your repository here The default is a processed version of project_name that is compatible with python package names author_name : The author of the project description : A short description of your project openness : Whether the project is \"public\" (default) or \"private\" You should only make a project private if you have a good reason If \"public\" then an MIT license will be generated; otherwise the license will be a copyright placeholder If you choose auto_config as \"true\" then the Github repo created will obey this setting s3_bucket : The name of an S3 bucket to store assets for this project If you choose auto_config as \"true\" then this bucket will be created for you Careful : This needs to not conflict with any existing s3 bucket name This value can be reconfigured in .env.shared github_account : The github account that this project will be created in auto_config : Whether to automatically create a conda environment; github repo; S3 bucket; and configure AWS with metaflow. Requires optional requirement gh (the Github cli) to have been installed and configured (with gh auth login )","title":"Create"},{"location":"quickstart/#configure","text":"If you selected auto_config as \"true\", the following actions have happened: A conda environment, project_name , has been created (with the project package installed in editable mode) Git pre-commit hooks have been configured and installed The Nesta metaflow config has been fetched and decrypted. It should exist in ~/.metaflowconfig/config.json A github repo github.com/nestauk/project_name has been created and configured An s3 bucket project_name has been created If you selected auto_config as \"false\", you will need to do the above manually (or run make init ): Run make install to configure the development environment: Setup the conda environment Configure pre-commit Configure metaflow to use AWS Manually create an S3 bucket s3_bucket (or run bash bin/create_bucket.sh ) S3 names must only contain lowercase letters, numbers, dots, and hyphens. Manually create a github repository (or run bash bin/create_repo.sh )","title":"Configure"},{"location":"quickstart/#collaborating-on-an-existing-project","text":"Clone the repository and cd into the repository. Run make install to configure the development environment: Setup the conda environment Install dependencies and local package Configure pre-commit Configure metaflow to use AWS conda activate project_name Check the project's README for any additional configuration needed (e.g. putting API keys in .env ) Pull any required inputs into inputs/ by running make inputs-pull Follow the author's documentation, or make them write some if they haven't already!","title":"Collaborating on an existing project"},{"location":"roadmap/","text":"Roadmap Metaflow In the coming weeks we will be rolling out utilities to make working with metaflow easier. For example, you will be able to specify the following YAML file and then run a command like run_flow sic_classifier and the corresponding metaflow pipeline will run on AWS batch with 8 CPU's, 64GB RAM with the configuration parameters specified in flow_kwargs . #file: src/config/pipeline/sic_classifier.yaml preflow_kwargs: with: batch,cpu=8,memory=64000 flow_kwargs: documents_path: inputs/data/descriptions.json freeze-model: false config: encode: add_special_tokens: true max_length: 64 pad_to_max_length: true Furthermore, successful runs will be tracked in version control allow data getters you write for metaflows to automatically fetch the right flow results. ds-utils Development of ds-utils will begin soon. This will be a package providing well tested and documented data science utilities based on our previous data-science projects. Reporting In a few recent projects we have been experimenting with a report workflow using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. After further refinement of this workflow and development of a simple report generation tool, this can be incorporated into the cookiecutter. flake8 Identification of a suitable starting flake8 configuration and a plan to phase in the number of cases handled by flake8 (to avoid an overwhelming start). Incorporation of flake8 into the pre-commit hooks. Note: you can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint . Poetry Investigate a switch to managing dependencies and packaging with poetry (falling back on Conda only when necessary). This will provide much more robust dependency management and a simplification of the packaging utilities. This tool is not being integrated into the overhauled cookiecutter from day one to avoid user overwhelm. Docker Investigate the utility of automatically containerisation data-science project. Schema Investigate the use of pydantic to document data-schemas and provide data-validation where required. This has the benefit of ensuring documentation stays up to date, and provides the ability to generate schemas in alternative forms such as converting to SQLalchemy and outputting in language-agnostic formats such as JSON schema . CI/CD Use Github actions to: - Automatically build a Docker container of the project - Automatically run tests - Perform pre-commit actions on the server to guard against user error Configuration Investigate options for Machine-learning oriented configuration management - e.g. with gin or the approach used by thinc .","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#metaflow","text":"In the coming weeks we will be rolling out utilities to make working with metaflow easier. For example, you will be able to specify the following YAML file and then run a command like run_flow sic_classifier and the corresponding metaflow pipeline will run on AWS batch with 8 CPU's, 64GB RAM with the configuration parameters specified in flow_kwargs . #file: src/config/pipeline/sic_classifier.yaml preflow_kwargs: with: batch,cpu=8,memory=64000 flow_kwargs: documents_path: inputs/data/descriptions.json freeze-model: false config: encode: add_special_tokens: true max_length: 64 pad_to_max_length: true Furthermore, successful runs will be tracked in version control allow data getters you write for metaflows to automatically fetch the right flow results.","title":"Metaflow"},{"location":"roadmap/#ds-utils","text":"Development of ds-utils will begin soon. This will be a package providing well tested and documented data science utilities based on our previous data-science projects.","title":"ds-utils"},{"location":"roadmap/#reporting","text":"In a few recent projects we have been experimenting with a report workflow using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. After further refinement of this workflow and development of a simple report generation tool, this can be incorporated into the cookiecutter.","title":"Reporting"},{"location":"roadmap/#flake8","text":"Identification of a suitable starting flake8 configuration and a plan to phase in the number of cases handled by flake8 (to avoid an overwhelming start). Incorporation of flake8 into the pre-commit hooks. Note: you can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint .","title":"flake8"},{"location":"roadmap/#poetry","text":"Investigate a switch to managing dependencies and packaging with poetry (falling back on Conda only when necessary). This will provide much more robust dependency management and a simplification of the packaging utilities. This tool is not being integrated into the overhauled cookiecutter from day one to avoid user overwhelm.","title":"Poetry"},{"location":"roadmap/#docker","text":"Investigate the utility of automatically containerisation data-science project.","title":"Docker"},{"location":"roadmap/#schema","text":"Investigate the use of pydantic to document data-schemas and provide data-validation where required. This has the benefit of ensuring documentation stays up to date, and provides the ability to generate schemas in alternative forms such as converting to SQLalchemy and outputting in language-agnostic formats such as JSON schema .","title":"Schema"},{"location":"roadmap/#cicd","text":"Use Github actions to: - Automatically build a Docker container of the project - Automatically run tests - Perform pre-commit actions on the server to guard against user error","title":"CI/CD"},{"location":"roadmap/#configuration","text":"Investigate options for Machine-learning oriented configuration management - e.g. with gin or the approach used by thinc .","title":"Configuration"},{"location":"structure/","text":"Structure This page lays out where things belong according to high-level concepts. A direct tree representation of the folder hierarchy is also available. Example structures will soon be available to help you structure the lower-level folders which the cookiecutter leaves to you. In the following sections I use src/ to denote the project name to avoid awkward <project_name> placeholders. Project configuration - Makefile We use make to manage tasks relating to project setup/configuration/recurring tasks. make is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms. Running make from the project base directory will document the commands available along with a short description. Available rules: clean Delete all compiled Python files conda-create Create a conda environment conda-update Update the conda-environment based on changes to `environment.yaml` docs Build the API documentation docs-clean Clean the built API documentation docs-open Open the docs in the browser init Fully initialise a project: install; setup github repo; setup S3 bucket inputs-pull Pull `inputs/` from S3 inputs-push Push `inputs/` to S3 (WARNING: this may overwrite existing files!) install Install a project: create conda env; install local package; setup git hooks; setup metaflow+AWS lint Run flake8 linting on repository pip-install Install our package and requirements in editable mode (including development dependencies) pre-commit Perform pre-commit actions Where appropriate these make commands will automatically be run in the conda environment for a project. Git hooks We use pre-commit to check the integrity of git commits before they happen. The steps are specified in .pre-commit-config.yaml . Currently the steps that are taken are: - Run the black code autoformatter - This provides a consistent code style across a project and minimises messy git diffs (sometimes the code formatted by black may look \"uglier\" in places but this is the price we pay for having an industry standard with minimal cognitive burden) - Check that no large files were accidentally committed - Check that there are no merge conflict strings (e.g. >>>>> ) lingering in files - Fix the end of files to work across operating systems - Trim trailing whitespace in files - Check Toml files are well formed - Check Yaml files are well formed - Check we are no committing directly to dev , master , or main - Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML) Warning: You need to run git commit with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: pre-commit install --install-hooks will install the pre-commit hooks in the currently active environment). In time we will be integrating flake8 into these pre-commit hooks. You can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint . Reproducable environment The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis, but what tool should we use to do that? Whilst popular for scientific computing and data-science, conda poses problems for collaboration and packaging: It is hard to reproduce a conda-environment across operating systems It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda Files Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through pip install (one prominent example of this is graph-tool ). environment.yaml - Defines the base conda environment and any dependencies not \"pip-installable\". requirements.txt - Defines the dependences required to run the code. If you need to add a dependency, chances are it goes here! requirements_dev.txt - Defines development dependencies. These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and ipykernel to run code in jupyter (It's likely that you never need to think about this file) Commands make conda-create - Create a conda environment from environment.yaml and run make pip-install . Note: this is automatically called by make install and make init but exists as a stand-alone command in case you ever need it make conda-update - Update an existing conda environment from environment.yaml and run make pip-install . make pip-install - Install our package and requirements in editable mode (including development dependencies). Roadmap See roadmap for plans on improving packaging and reproducibility with Poetry and Docker . Secrets and configuration - .env.* and src/config/* You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point. Store your secrets in a special file Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname OTHER_VARIABLE=something You can use python-dotenv to load the entries as follows: import os from dotenv import load_dotenv, find_dotenv load_dotenv(find_dotenv()) # Find .env and load entries print(os.getenv(\"DATABASE_URL\")) print(os.getenv(\"SOME_VARIABLE_NOT_IN_ENV_FILE\")) # >> postgres://username:password@localhost:5432/dbname # >> None We also have .env.shared which contains non-secret project configuration variables that are used for example by commands in our Makefile Store Data-science configuration in src/config/ Few things scupper colloborative analysis like hard-coding hyper-parameters parameters deep in the code-base. src/config/base.yaml provides a place to document choices made. For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following, patstat_companies_house: match_threshold: 90 and load that value into your code with, from src import config config[\"patstat_companies_house\"][\"match_threshold\"] This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase. Aside - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. /home/alex/GIT/my_fantastic_data_project/outputs/data/foo.json , this will never work on anything other than your machine. Instead use relative paths and make use of src.PROJECT_DIR which will return the path to your project's base directory. This means you could specify the above path as f\"{src.PROJECT_DIR}/outputs/data/foo.json\" and have it work on everyone's machine! Roadmap See the roadmap for how src/config will be used to parameterise metaflow pipelines and version control their outputs. Data - inputs/data , outputs/data , outputs/.cache Firstly, don't version control data (inputs or outputs) in git, generally you should use s3 (directly or through metaflow) to manage your data. inputs/data Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial). Don't ever edit this raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. Store it in AWS S3 . When the project was configured, you will have been prompted for a BUCKET variable (now tracked in .env.shared ). If you used the auto_config option, an S3 bucket will have been setup for you too. Two make commands - make inputs-pull and make inputs-push - can be used to push and pull data from the configured s3 bucket. outputs/.cache/ This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents. For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in outputs/data ; or to temporary data when prototyping. outputs/data This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project. Fetching/loading data - src/getters This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks). This means that peppering calls like pd.read_csv(\"path/to/file\", sep=\"\\t\", ...) throughout the codebase should be strictly avoided. Following this approach means: If the format of path/to/file changes then we only have to make the change in one place We avoid inconsistencies such as forgetting to read a column in as a str instead of an int and thus missing leading zeroes. If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to pd.read_csv like we had above # File: getters/companies_house.py \"\"\"Data getters for the companies house data. Data source: https://download.companieshouse.gov.uk/en_output.html \"\"\" import pandas as pd def get_sector() -> pd.DataFrame: \"\"\"Load Companies House sector labels. Returns: Sector information for ... \"\"\" return pd.read_csv(\"path/to/file\", sep=\"\\t\", dtype={\"sic_code\": str}) Roadmap On the roadmap is a speculative plan to explore the use of pydantic to specify and validate data-schemas. Pipeline components - src/pipeline This folder contains pipeline components. Put as much data-science as possible here. We recommend the use of metaflow to write these pipeline components. In the coming months as we roll out utilities and documentation to smooth out some of the rough edges of metaflow , this will become less of a recommendation and more of a stipulation. Using metaflow: - Gives us lightweight version control of data and models - Gives us easy access to AWS batch computing (including GPU machines) - Makes it easy to take data-science code into production Shared utilities - src/utils This is a place to put utility functions needed across different parts of the codebase. For example, this could be functions shared across different pieces of analysis or different pipelines. Roadmap Over time there should be a decreasing need to add things to utils as we begin to develop a data science utilities package ( ds-utils ) . Analysis - src/analysis Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports. It is easier to say when shomething shouldn't be in analysis than when something should: If one part in analysis depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of analysis should be completely independent). It is important that plots are persisted to disk (in outputs/figures ). Notebooks Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules. Where does the humble notebook live? Notebooks should be placed as close to the place where their functionality will eventually reside as possible. For example, if you are prototyping a \"sentence transformer\" pipeline then that pipeline component will likely end up somewhere like pipeline/sentence_transformer/ , therefore you should place the notebooks for prototyping this features in pipeline/sentence_transformer/notebooks/ . If you're just getting started with a project and don't have a clear sense of the separation between analysis , pipeline , and getters yet (or it's too premature to split functionality across multiple places) then a sensible place to start is analysis/<high-level-description>/notebooks/ . Version control Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), we use jupytext to pair .ipynb files with a human-readable and git-diffable .py file. These paired .py files should be committed to git, .ipynb files are git-ignored. To ensure jupytext works correctly you should start jupyter (notebook/lab) from the base directory of the project so that jupyter detects the jupytext configuration that lives in jupytext.toml . Refactoring Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); where as others prefer to keep everything in notebooks until the last minute. We do not require you to work one way or the other as long as by the time you submit a pull request (PR) for your feature everything is refactored into python modules. Having said this, we recommended you frequently refactor the good parts - you'll thank yourself later! A warning sign you've left it too late to refactor is if you've got duplicates of functions across the codebase rather than importing from a logical place - if it's a data preprocessing task, put it in the pipeline at src/pipelines/<descriptive name for task> ; if it's useful utility code, refactor it to src/utils/ ; if it's loading data, refactor it to src/getters . Tips Add the following to your notebook (or IPython REPL): %load_ext autoreload %autoreload 2 Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc. When it comes to refactoring, open the python file Jupytext pairs to your notebook in your editor of choice - now your notebook code is easily-readable and in the same environment you use to write python modules. Share with gists As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token . Don't install jupyter / jupyterlab in your environment, use ipykernel You should avoid jupyter / jupyterlab as a dependency in the project environment. Instead add ipykernel as a dependency. This is a lightweight dependency that allows jupyter / jupyterlab installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project. The advantages of this are: You only have to configure jupyter / jupyterlab once You will save disk-space Faster install Colleagues using other editors don't have to install heavy dependencies they don't use (you wouldn't be happy if someone sent you code that depended on VScode/pycharm/spyder) Note: ipykernel is also listed in requirements_dev.txt so you do not need to add it. Report - outputs/reports We are currently evaluating how we report data-science work - both at the project-level and the feature-level. Minimally, you should write reports in markdown putting them in outputs/reports and referencing plots in outputs/figures . We are experimenting with a toolchain using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs. Tree \u251c\u2500\u2500 bin | PROJECT CONFIGURATION SCRIPTS \u2502 \u251c\u2500\u2500 conda_activate.sh | Helper to activate conda in shell environment \u2502 \u251c\u2500\u2500 create_bucket.sh | Create S3 bucket \u2502 \u251c\u2500\u2500 create_repo.sh | Create Github repo \u2502 \u2514\u2500\u2500 install_metaflow_aws.sh | Configure Metaflow with AWS \u251c\u2500\u2500 <repo_name> | PYTHON PACKAGE \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 analysis | Analysis \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 config | Configuration \u2502 \u2502 \u251c\u2500\u2500 logging.yaml | logging configuration \u2502 \u2502 \u251c\u2500\u2500 base.yaml | global configuration (e.g. for tracking hyper-parameters) \u2502 \u2502 \u2514\u2500\u2500 pipeline | pipeline configuration files \u2502 \u2502 \u2514\u2500\u2500 .gitkeep | \u2502 \u251c\u2500\u2500 getters | Data getters \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 pipeline | Pipeline components \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u2514\u2500\u2500 utils | Utilities \u2502 \u2514\u2500\u2500 __init__.py | \u251c\u2500\u2500 docs | DOCUMENTATION \u2502 \u251c\u2500\u2500 conf.py | Configures docs \u2502 \u251c\u2500\u2500 index.rst | \u2502 \u2514\u2500\u2500 license.rst | \u251c\u2500\u2500 environment.yaml | CONDA ENVIRONMENT SPECIFICATION (optional component) \u251c\u2500\u2500 requirements.txt | PYTHON DEPENDENCIES NEEDED TO RUN THE CODE \u251c\u2500\u2500 requirements_dev.txt | PYTHON DEV DEPENDENCIES (e.g. building docs/running tests) \u251c\u2500\u2500 inputs | INPUTS (should be immutable) \u2502 \u251c\u2500\u2500 data | Inputs that are data \u2502 \u251c\u2500\u2500 models | Inputs that are models \u2502 \u2514\u2500\u2500 README.md | \u251c\u2500\u2500 jupytext.toml | JUPYTEXT CONFIGURATION \u251c\u2500\u2500 LICENSE | \u251c\u2500\u2500 outputs | OUTPUTS PRODUCED FROM THE PROJECT \u2502 \u251c\u2500\u2500 data | Data outputs (from running our code) \u2502 \u251c\u2500\u2500 figures | Figure outputs (from running our code) \u2502 \u2502 \u2514\u2500\u2500 vegalite | JSON specification of altair/vegalite figures \u2502 \u251c\u2500\u2500 models | Model outputs (from running our code) \u2502 \u2514\u2500\u2500 reports | Reports about our code and the results of running it \u251c\u2500\u2500 Makefile | TASKS TO COORDINATE PROJECT (`make` shows available commands) \u251c\u2500\u2500 README.md | \u251c\u2500\u2500 setup.py | ALLOWS US TO PIP INSTALL src/ \u251c\u2500\u2500 setup.cfg | ADDITIONAL PROJECT CONFIGURATION, e.g. linting \u251c\u2500\u2500 .pre-commit-cofig.yaml | DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS \u251c\u2500\u2500 .gitignore | TELLS git WHAT FILES WE DON'T WANT TO COMMIT \u251c\u2500\u2500 .github | GITHUB CONFIGURATION \u2502 \u2514\u2500\u2500 pull_request_template.md | Template for pull-requests (check-list of things to do) \u251c\u2500\u2500 .env | SECRETS (never commit to git!) \u251c\u2500\u2500 .env.shared | SHARED PROJECT CONFIGURATION VARIABLES","title":"Structure"},{"location":"structure/#structure","text":"This page lays out where things belong according to high-level concepts. A direct tree representation of the folder hierarchy is also available. Example structures will soon be available to help you structure the lower-level folders which the cookiecutter leaves to you. In the following sections I use src/ to denote the project name to avoid awkward <project_name> placeholders.","title":"Structure"},{"location":"structure/#project-configuration-makefile","text":"We use make to manage tasks relating to project setup/configuration/recurring tasks. make is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms. Running make from the project base directory will document the commands available along with a short description. Available rules: clean Delete all compiled Python files conda-create Create a conda environment conda-update Update the conda-environment based on changes to `environment.yaml` docs Build the API documentation docs-clean Clean the built API documentation docs-open Open the docs in the browser init Fully initialise a project: install; setup github repo; setup S3 bucket inputs-pull Pull `inputs/` from S3 inputs-push Push `inputs/` to S3 (WARNING: this may overwrite existing files!) install Install a project: create conda env; install local package; setup git hooks; setup metaflow+AWS lint Run flake8 linting on repository pip-install Install our package and requirements in editable mode (including development dependencies) pre-commit Perform pre-commit actions Where appropriate these make commands will automatically be run in the conda environment for a project.","title":"Project configuration - Makefile"},{"location":"structure/#git-hooks","text":"We use pre-commit to check the integrity of git commits before they happen. The steps are specified in .pre-commit-config.yaml . Currently the steps that are taken are: - Run the black code autoformatter - This provides a consistent code style across a project and minimises messy git diffs (sometimes the code formatted by black may look \"uglier\" in places but this is the price we pay for having an industry standard with minimal cognitive burden) - Check that no large files were accidentally committed - Check that there are no merge conflict strings (e.g. >>>>> ) lingering in files - Fix the end of files to work across operating systems - Trim trailing whitespace in files - Check Toml files are well formed - Check Yaml files are well formed - Check we are no committing directly to dev , master , or main - Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML) Warning: You need to run git commit with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: pre-commit install --install-hooks will install the pre-commit hooks in the currently active environment). In time we will be integrating flake8 into these pre-commit hooks. You can already lint your code using (a quite opinionated) flake8 configuration defined in setup.cfg by running make lint .","title":"Git hooks"},{"location":"structure/#reproducable-environment","text":"The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together. By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis, but what tool should we use to do that? Whilst popular for scientific computing and data-science, conda poses problems for collaboration and packaging: It is hard to reproduce a conda-environment across operating systems It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda","title":"Reproducable environment"},{"location":"structure/#files","text":"Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through pip install (one prominent example of this is graph-tool ). environment.yaml - Defines the base conda environment and any dependencies not \"pip-installable\". requirements.txt - Defines the dependences required to run the code. If you need to add a dependency, chances are it goes here! requirements_dev.txt - Defines development dependencies. These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and ipykernel to run code in jupyter (It's likely that you never need to think about this file)","title":"Files"},{"location":"structure/#commands","text":"make conda-create - Create a conda environment from environment.yaml and run make pip-install . Note: this is automatically called by make install and make init but exists as a stand-alone command in case you ever need it make conda-update - Update an existing conda environment from environment.yaml and run make pip-install . make pip-install - Install our package and requirements in editable mode (including development dependencies).","title":"Commands"},{"location":"structure/#roadmap","text":"See roadmap for plans on improving packaging and reproducibility with Poetry and Docker .","title":"Roadmap"},{"location":"structure/#secrets-and-configuration-env-and-srcconfig","text":"You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point.","title":"Secrets and configuration - .env.* and src/config/*"},{"location":"structure/#store-your-secrets-in-a-special-file","text":"Create a .env file in the project root folder. Thanks to the .gitignore , this file should never get committed into the version control repository. Here's an example: # example .env file DATABASE_URL=postgres://username:password@localhost:5432/dbname OTHER_VARIABLE=something You can use python-dotenv to load the entries as follows: import os from dotenv import load_dotenv, find_dotenv load_dotenv(find_dotenv()) # Find .env and load entries print(os.getenv(\"DATABASE_URL\")) print(os.getenv(\"SOME_VARIABLE_NOT_IN_ENV_FILE\")) # >> postgres://username:password@localhost:5432/dbname # >> None We also have .env.shared which contains non-secret project configuration variables that are used for example by commands in our Makefile","title":"Store your secrets in a special file"},{"location":"structure/#store-data-science-configuration-in-srcconfig","text":"Few things scupper colloborative analysis like hard-coding hyper-parameters parameters deep in the code-base. src/config/base.yaml provides a place to document choices made. For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following, patstat_companies_house: match_threshold: 90 and load that value into your code with, from src import config config[\"patstat_companies_house\"][\"match_threshold\"] This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase. Aside - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. /home/alex/GIT/my_fantastic_data_project/outputs/data/foo.json , this will never work on anything other than your machine. Instead use relative paths and make use of src.PROJECT_DIR which will return the path to your project's base directory. This means you could specify the above path as f\"{src.PROJECT_DIR}/outputs/data/foo.json\" and have it work on everyone's machine!","title":"Store Data-science configuration in src/config/"},{"location":"structure/#roadmap_1","text":"See the roadmap for how src/config will be used to parameterise metaflow pipelines and version control their outputs.","title":"Roadmap"},{"location":"structure/#data-inputsdata-outputsdata-outputscache","text":"Firstly, don't version control data (inputs or outputs) in git, generally you should use s3 (directly or through metaflow) to manage your data.","title":"Data - inputs/data, outputs/data, outputs/.cache"},{"location":"structure/#inputsdata","text":"Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial). Don't ever edit this raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. Store it in AWS S3 . When the project was configured, you will have been prompted for a BUCKET variable (now tracked in .env.shared ). If you used the auto_config option, an S3 bucket will have been setup for you too. Two make commands - make inputs-pull and make inputs-push - can be used to push and pull data from the configured s3 bucket.","title":"inputs/data"},{"location":"structure/#outputscache","text":"This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents. For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in outputs/data ; or to temporary data when prototyping.","title":"outputs/.cache/"},{"location":"structure/#outputsdata","text":"This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project.","title":"outputs/data"},{"location":"structure/#fetchingloading-data-srcgetters","text":"This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks). This means that peppering calls like pd.read_csv(\"path/to/file\", sep=\"\\t\", ...) throughout the codebase should be strictly avoided. Following this approach means: If the format of path/to/file changes then we only have to make the change in one place We avoid inconsistencies such as forgetting to read a column in as a str instead of an int and thus missing leading zeroes. If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to pd.read_csv like we had above # File: getters/companies_house.py \"\"\"Data getters for the companies house data. Data source: https://download.companieshouse.gov.uk/en_output.html \"\"\" import pandas as pd def get_sector() -> pd.DataFrame: \"\"\"Load Companies House sector labels. Returns: Sector information for ... \"\"\" return pd.read_csv(\"path/to/file\", sep=\"\\t\", dtype={\"sic_code\": str})","title":"Fetching/loading data - src/getters"},{"location":"structure/#roadmap_2","text":"On the roadmap is a speculative plan to explore the use of pydantic to specify and validate data-schemas.","title":"Roadmap"},{"location":"structure/#pipeline-components-srcpipeline","text":"This folder contains pipeline components. Put as much data-science as possible here. We recommend the use of metaflow to write these pipeline components. In the coming months as we roll out utilities and documentation to smooth out some of the rough edges of metaflow , this will become less of a recommendation and more of a stipulation. Using metaflow: - Gives us lightweight version control of data and models - Gives us easy access to AWS batch computing (including GPU machines) - Makes it easy to take data-science code into production","title":"Pipeline components - src/pipeline"},{"location":"structure/#shared-utilities-srcutils","text":"This is a place to put utility functions needed across different parts of the codebase. For example, this could be functions shared across different pieces of analysis or different pipelines.","title":"Shared utilities - src/utils"},{"location":"structure/#roadmap_3","text":"Over time there should be a decreasing need to add things to utils as we begin to develop a data science utilities package ( ds-utils ) .","title":"Roadmap"},{"location":"structure/#analysis-srcanalysis","text":"Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports. It is easier to say when shomething shouldn't be in analysis than when something should: If one part in analysis depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of analysis should be completely independent). It is important that plots are persisted to disk (in outputs/figures ).","title":"Analysis - src/analysis"},{"location":"structure/#notebooks","text":"Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules.","title":"Notebooks"},{"location":"structure/#where-does-the-humble-notebook-live","text":"Notebooks should be placed as close to the place where their functionality will eventually reside as possible. For example, if you are prototyping a \"sentence transformer\" pipeline then that pipeline component will likely end up somewhere like pipeline/sentence_transformer/ , therefore you should place the notebooks for prototyping this features in pipeline/sentence_transformer/notebooks/ . If you're just getting started with a project and don't have a clear sense of the separation between analysis , pipeline , and getters yet (or it's too premature to split functionality across multiple places) then a sensible place to start is analysis/<high-level-description>/notebooks/ .","title":"Where does the humble notebook live?"},{"location":"structure/#version-control","text":"Since notebooks are challenging objects for source control (e.g., diffs of the json are often not human-readable and merging is a nightmare), we use jupytext to pair .ipynb files with a human-readable and git-diffable .py file. These paired .py files should be committed to git, .ipynb files are git-ignored. To ensure jupytext works correctly you should start jupyter (notebook/lab) from the base directory of the project so that jupyter detects the jupytext configuration that lives in jupytext.toml .","title":"Version control"},{"location":"structure/#refactoring","text":"Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); where as others prefer to keep everything in notebooks until the last minute. We do not require you to work one way or the other as long as by the time you submit a pull request (PR) for your feature everything is refactored into python modules. Having said this, we recommended you frequently refactor the good parts - you'll thank yourself later! A warning sign you've left it too late to refactor is if you've got duplicates of functions across the codebase rather than importing from a logical place - if it's a data preprocessing task, put it in the pipeline at src/pipelines/<descriptive name for task> ; if it's useful utility code, refactor it to src/utils/ ; if it's loading data, refactor it to src/getters .","title":"Refactoring"},{"location":"structure/#tips","text":"Add the following to your notebook (or IPython REPL): %load_ext autoreload %autoreload 2 Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc. When it comes to refactoring, open the python file Jupytext pairs to your notebook in your editor of choice - now your notebook code is easily-readable and in the same environment you use to write python modules.","title":"Tips"},{"location":"structure/#share-with-gists","text":"As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires jupyter_nbextensions_configurator and a github personal access token .","title":"Share with gists"},{"location":"structure/#dont-install-jupyterjupyterlab-in-your-environment-use-ipykernel","text":"You should avoid jupyter / jupyterlab as a dependency in the project environment. Instead add ipykernel as a dependency. This is a lightweight dependency that allows jupyter / jupyterlab installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project. The advantages of this are: You only have to configure jupyter / jupyterlab once You will save disk-space Faster install Colleagues using other editors don't have to install heavy dependencies they don't use (you wouldn't be happy if someone sent you code that depended on VScode/pycharm/spyder) Note: ipykernel is also listed in requirements_dev.txt so you do not need to add it.","title":"Don't install jupyter/jupyterlab in your environment, use ipykernel"},{"location":"structure/#report-outputsreports","text":"We are currently evaluating how we report data-science work - both at the project-level and the feature-level. Minimally, you should write reports in markdown putting them in outputs/reports and referencing plots in outputs/figures . We are experimenting with a toolchain using pandoc to generate HTML and PDF (LaTeX) outputs from a single ( pandoc flavoured ) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs.","title":"Report - outputs/reports"},{"location":"structure/#tree","text":"\u251c\u2500\u2500 bin | PROJECT CONFIGURATION SCRIPTS \u2502 \u251c\u2500\u2500 conda_activate.sh | Helper to activate conda in shell environment \u2502 \u251c\u2500\u2500 create_bucket.sh | Create S3 bucket \u2502 \u251c\u2500\u2500 create_repo.sh | Create Github repo \u2502 \u2514\u2500\u2500 install_metaflow_aws.sh | Configure Metaflow with AWS \u251c\u2500\u2500 <repo_name> | PYTHON PACKAGE \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 analysis | Analysis \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 config | Configuration \u2502 \u2502 \u251c\u2500\u2500 logging.yaml | logging configuration \u2502 \u2502 \u251c\u2500\u2500 base.yaml | global configuration (e.g. for tracking hyper-parameters) \u2502 \u2502 \u2514\u2500\u2500 pipeline | pipeline configuration files \u2502 \u2502 \u2514\u2500\u2500 .gitkeep | \u2502 \u251c\u2500\u2500 getters | Data getters \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 pipeline | Pipeline components \u2502 \u2502 \u2514\u2500\u2500 __init__.py | \u2502 \u2514\u2500\u2500 utils | Utilities \u2502 \u2514\u2500\u2500 __init__.py | \u251c\u2500\u2500 docs | DOCUMENTATION \u2502 \u251c\u2500\u2500 conf.py | Configures docs \u2502 \u251c\u2500\u2500 index.rst | \u2502 \u2514\u2500\u2500 license.rst | \u251c\u2500\u2500 environment.yaml | CONDA ENVIRONMENT SPECIFICATION (optional component) \u251c\u2500\u2500 requirements.txt | PYTHON DEPENDENCIES NEEDED TO RUN THE CODE \u251c\u2500\u2500 requirements_dev.txt | PYTHON DEV DEPENDENCIES (e.g. building docs/running tests) \u251c\u2500\u2500 inputs | INPUTS (should be immutable) \u2502 \u251c\u2500\u2500 data | Inputs that are data \u2502 \u251c\u2500\u2500 models | Inputs that are models \u2502 \u2514\u2500\u2500 README.md | \u251c\u2500\u2500 jupytext.toml | JUPYTEXT CONFIGURATION \u251c\u2500\u2500 LICENSE | \u251c\u2500\u2500 outputs | OUTPUTS PRODUCED FROM THE PROJECT \u2502 \u251c\u2500\u2500 data | Data outputs (from running our code) \u2502 \u251c\u2500\u2500 figures | Figure outputs (from running our code) \u2502 \u2502 \u2514\u2500\u2500 vegalite | JSON specification of altair/vegalite figures \u2502 \u251c\u2500\u2500 models | Model outputs (from running our code) \u2502 \u2514\u2500\u2500 reports | Reports about our code and the results of running it \u251c\u2500\u2500 Makefile | TASKS TO COORDINATE PROJECT (`make` shows available commands) \u251c\u2500\u2500 README.md | \u251c\u2500\u2500 setup.py | ALLOWS US TO PIP INSTALL src/ \u251c\u2500\u2500 setup.cfg | ADDITIONAL PROJECT CONFIGURATION, e.g. linting \u251c\u2500\u2500 .pre-commit-cofig.yaml | DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS \u251c\u2500\u2500 .gitignore | TELLS git WHAT FILES WE DON'T WANT TO COMMIT \u251c\u2500\u2500 .github | GITHUB CONFIGURATION \u2502 \u2514\u2500\u2500 pull_request_template.md | Template for pull-requests (check-list of things to do) \u251c\u2500\u2500 .env | SECRETS (never commit to git!) \u251c\u2500\u2500 .env.shared | SHARED PROJECT CONFIGURATION VARIABLES","title":"Tree"}]}